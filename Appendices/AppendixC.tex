% !TEX root = ../main.tex
% Appendix C

\chapter{Summary of papers and principle formulas}\label{AppendixC}
\lhead{Appendix C. \emph{Paper summaries and formulas}}

\section{Change point detection in time series data using support vectors, by Camci}
Paper: \cite{camci2010change}.
The main concept of this paper is to construct a hypersphere around the data and thereby generating a boundary.
A change point is detected when the radius of the hypersphere grows or shrinks significantly, or when a data point falls outside the boundary.

The main cost function being minimized:
%
\begin{equation}
  \minimize{r}
    {r^2 + C \sum_{i} \xi_i \norm{x}}
    {\norm{\vectorsym{x}_i - c}^2 \leq r^2 + \xi_i, \quad \xi_i \geq 0 \quad \forall i, \vectorsym{x}_i: i\text{th data point}}
\end{equation}
%
Where $r$ is the radius of a (hyper)circle with center $\vectorsym{c}$, $C$ is the penalty coefficient for every outlier and $\xi_i$ is the distance from the $i$th data point to hypersphere (also known as the slack variable).

The dual form by introducing the Lagrange multipliers ($\alpha_i, \alpha_i \geq 0$) and eliminating the slack variables $\xi$ is:
%
\begin{equation}
  \maximize{\vectorsym{\alpha}}
    {\sum_i \alpha_i (\vectorsym{x}_i \cdot \vectorsym{x}_i) - \sum_{i,j} \alpha_i \alpha_j (\vectorsym{x}_i \cdot \vectorsym{x}_j)}
    {0 \leq \alpha_i \leq C \quad\forall i, \quad \sum_i \alpha_i = 1}
\end{equation}
%
To allow for a non-linear relation between the data points and the data boundary, the inner product can be replaced by a (\eg Gaussian) kernel function: $K(\vectorsym{x}_i, \vectorsym{x}_j)$.


%---------------------------------------------
%---------------------------------------------
\section{Change-Point detection in time-series data by Direct Density-Ratio Estimation}
Paper: \cite{kawahara2009change}.

The density ratio $w(\matrixsym{Y})$ is modeled by a Gaussian kernel over sequences $\matrixsym{Y}$ of samples (sequence $\matrixsym{Y}_{te}(l)$ is the test sequence from the $l$th position on):
%
\begin{equation}
  \hat{w}(\matrixsym{Y}) = \sum_{l=1}^{n_\text{te}} \alpha_l K_\sigma(\matrixsym{Y}, \matrixsym{Y}_\text{te}(l)),
\end{equation}
%
where $\{\alpha_l\}_{l=1}^{n_\text{te}}$ are parameters to be learned from the data samples and $K_\sigma(\matrixsym{Y}, \matrixsym{Y}')$ is the Gaussian kernel function with mean $\matrixsym{Y}'$ and standard deviation $\sigma$.
The learned parameters minimize the Kullback-Leibler divergence from the sequence to the test sequence.
The maximization problem then becomes:
%
\begin{equation}
  \maximize{\{\alpha_l\}_{l=1}^{n_\text{te}}}
    {\sum_{i=1}^{n_\text{te}} \text{log} \left(\sum_{l=1}^{n_\text{te}} \alpha_l K_\sigma(\matrixsym{Y}_\text{te}(i), \matrixsym{Y}_\text{te}(l) ) \right)}
    {\frac{1}{n_\text{rf}} \sum_{i=1}^{n_\text{rf}} \sum_{l=1}^{n_\text{te}} \alpha_l K_\sigma(\matrixsym{Y}_\text{rf}(i), \matrixsym{Y}_\text{te}(l)) = 1, ~ \text{and} ~ \alpha_1, \dots ,\alpha_{n_\text{te}} \geq 1}
\end{equation}
%
With the estimated parameters the logarithm of the likelihood ratio between the test and reference interval can be calculated, which signals a change point if it is beyond a certain threshold $\mu$:
%
\begin{equation}
  S = \sum_{i=1}^{n_\text{te}} \text{ln}
    \frac
    {p_\text{te}(\matrixsym{Y}_\text{te}(i))}
    {p_\text{rf}(\matrixsym{Y}_\text{te}(i))}
\end{equation}


%---------------------------------------------
%---------------------------------------------
\section{Joint segmentation of multivariate time series with hidden process regression for human activity recognition, by Chamroukhi}
Paper: \cite{chamroukhi2013joint}.
This approach models the time series data by a parameterized regression, filtered with a \gls{hmm} to smooth out high frequency activity transitions.
With each observation $i$, generated by a $K$-state hidden process, an activity label $z_i$ (and thus sequence) is associated.
Observations follow a regression model:
%
\begin{equation}
  y_i = \beta_{z_i}^T \vectorsym{t}_i + \sigma_{z_i} \epsilon_i; \quad \epsilon_i \sim \mathcal{N}(0,1), \quad (i=1, \dots ,n)
\end{equation}
%
The observations get a label assigned by maximizing the logistic probability ($\pi_k$):
%
\begin{equation}
  \hat{z}_i = \argmaxold \limits_{1 \leq k \leq K} \pi_k (t_i; \hat{\vectorsym{w}}), \quad (i=1, \dots, n)
\end{equation}
%