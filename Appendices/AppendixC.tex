% !TEX root = ../main.tex
% Appendix C

\chapter{Summary of papers and principle formulas}\label{AppendixC}
\lhead{Appendix C. \emph{Paper summaries and formulas}}

\section{Change point detection in time series data using support vectors, by Camci}\label{sec:camci}
Paper: \cite{camci2010change}.
The main concept of this paper is to construct a hypersphere around the data and thereby generating a boundary.
A change point is detected when the radius of the hypersphere grows or shrinks significantly, or when a data point falls outside the boundary.

The main cost function being minimized:
%
\begin{equation}
  \minimize{r}
    {r^2 + C \sum_{i} \xi_i \norm{x}}
    {\norm{\vectorsym{x}_i - c}^2 \leq r^2 + \xi_i, \quad \xi_i \geq 0 \quad \forall i, \vectorsym{x}_i: i\text{th data point}}
\end{equation}
%
Where $r$ is the radius of a (hyper)circle with center $\vectorsym{c}$, $C$ is the penalty coefficient for every outlier and $\xi_i$ is the distance from the $i$th data point to hypersphere (also known as the slack variable).

The dual form by introducing the Lagrange multipliers ($\alpha_i, \alpha_i \geq 0$) and eliminating the slack variables $\xi$ is:
%
\begin{equation}
  \maximize{\vectorsym{\alpha}}
    {\sum_i \alpha_i (\vectorsym{x}_i \cdot \vectorsym{x}_i) - \sum_{i,j} \alpha_i \alpha_j (\vectorsym{x}_i \cdot \vectorsym{x}_j)}
    {0 \leq \alpha_i \leq C \quad\forall i, \quad \sum_i \alpha_i = 1}
\end{equation}
%
To allow for a non-linear relation between the data points and the data boundary, the inner product can be replaced by a (\eg Gaussian) kernel function: $K(\vectorsym{x}_i, \vectorsym{x}_j)$.





%---------------------------------------------
%---------------------------------------------
\clearpage
\section{Change-Point detection in time-series data by Direct Density-Ratio Estimation}
Paper: \cite{kawahara2009change}.

The density ratio $w(\matrixsym{Y})$ is modeled by a Gaussian kernel over sequences $\matrixsym{Y}$ of samples (sequence $\matrixsym{Y}_{te}(l)$ is the test sequence from the $l$th position on):
%
\begin{equation}
  \hat{w}(\matrixsym{Y}) = \sum_{l=1}^{n_\text{te}} \alpha_l K_\sigma(\matrixsym{Y}, \matrixsym{Y}_\text{te}(l)),
\end{equation}
%
where $\{\alpha_l\}_{l=1}^{n_\text{te}}$ are parameters to be learned from the data samples and $K_\sigma(\matrixsym{Y}, \matrixsym{Y}')$ is the Gaussian kernel function with mean $\matrixsym{Y}'$ and standard deviation $\sigma$.
The learned parameters minimize the Kullback-Leibler divergence from the sequence to the test sequence.
The maximization problem then becomes:
%
\begin{equation}
  \maximize{\{\alpha_l\}_{l=1}^{n_\text{te}}}
    {\sum_{i=1}^{n_\text{te}} \text{log} \left(\sum_{l=1}^{n_\text{te}} \alpha_l K_\sigma(\matrixsym{Y}_\text{te}(i), \matrixsym{Y}_\text{te}(l) ) \right)}
    {\frac{1}{n_\text{rf}} \sum_{i=1}^{n_\text{rf}} \sum_{l=1}^{n_\text{te}} \alpha_l K_\sigma(\matrixsym{Y}_\text{rf}(i), \matrixsym{Y}_\text{te}(l)) = 1, ~ \text{and} ~ \alpha_1, \dots ,\alpha_{n_\text{te}} \geq 1}
\end{equation}
%
With the estimated parameters the logarithm of the likelihood ratio between the test and reference interval can be calculated, which signals a change point if it is beyond a certain threshold $\mu$:
%
\begin{equation}
  S = \sum_{i=1}^{n_\text{te}} \text{ln}
    \frac
    {p_\text{te}(\matrixsym{Y}_\text{te}(i))}
    {p_\text{rf}(\matrixsym{Y}_\text{te}(i))}
\end{equation}







%---------------------------------------------
%---------------------------------------------
\clearpage
\section{Joint segmentation of multivariate time series with hidden process regression for human activity recognition, by Chamroukhi}
Paper: \cite{chamroukhi2013joint}.
This approach models the time series data by a parameterized regression, filtered with a \gls{hmm} to smooth out high frequency activity transitions.
With each observation $i$, generated by a $K$-state hidden process, an activity label $z_i$ (and thus sequence) is associated.
Observations follow a regression model:
%
\begin{equation}
  y_i = \beta_{z_i}^T \vectorsym{t}_i + \sigma_{z_i} \epsilon_i; \quad \epsilon_i \sim \mathcal{N}(0,1), \quad (i=1, \dots ,n)
\end{equation}
%
The observations get a label assigned by maximizing the logistic probability ($\pi_k$):
%
\begin{equation}
  \hat{z}_i = \argmaxold \limits_{1 \leq k \leq K} \pi_k (t_i; \hat{\vectorsym{w}}), \quad (i=1, \dots, n)
\end{equation}
%






%---------------------------------------------
%---------------------------------------------
\clearpage
\section{Support Vector Data Description, by Tax and Duin}
Paper: \cite{tax2004support}.
The paper proposes the \gls{svdd} method, analogous to the \gls{svc} of Vapnik \cite{vapnik1998statistical}, based on the separating hyperplane of Sch{\"o}lkopf \etal \cite{scholkopf1999sv}.
Where \gls{svc} is ablo to distinguish data between two classes, \gls{svdd} obtains a closed boundary around the target class and can detect outliers.

Method and formulas very similair to description in Section \ref{sec:camci}.






%---------------------------------------------
%---------------------------------------------
\clearpage
\section{Support Vector Density Estimation, by Weston et al.}
Paper: \cite{weston1999support}.
Using the notation of \cite{weston1999support}, the distribution function of a density function $p(x)$ is represented as:
%
\begin{equation}
  F(x) = P(X \leq x) = \int_{- \infty}^x p(t)\,\mathrm{d}t
\end{equation}
%
To find the density the following linear equation need to be solved:
%
\begin{equation}
  \int_{-\infty}^\infty \theta(x-t)p(t)\,\mathrm{d}t = F(x)
\end{equation}
%
where
%
\[
  \theta(x) =
  \begin{cases}
    1,& x > 0 \\
    0,& otherwise
  \end{cases}
\]
%
In this problem the distribution function $F(x)$ is unknown and instead we are given the \gls{idd} data $x_1,\dots,x_l$ generated by $F$.

The empirical distribution function can now be constructed as:
%
\begin{equation}
  F_l(x) = \frac{1}{l} \sum_{i=1}^l \theta(x-x_i)
\end{equation}




%---------------------------------------------
%---------------------------------------------
\clearpage
\section{An online algorithm for segmenting time series, by Keogh et al.}
Paper: \cite{keogh2001online}.
This method approximates the signal with piecewise linear representation.
Change points are encountered at the time at which a new segment is used to represent the signal.
The method uses linear regression by taking the best fitting line in the least squares sense, since that minimizes the Euclidian distance which is used as a quality metric.
Linear interpolation is considered but since that always has a greater sum of squares error it is disregarded.

Linear regression assumes a relation from $n$ observations $\matrixsym{X}$ to the dependend variable $\vectorsym{y}$ using the parameter vector $\vectorsym{\beta}$:
%
\begin{equation}
  \vectorsym{y} = \matrixsym{X}\vectorsym{\beta} + \epsilon
\end{equation}
%
%
The error function, the sum of squared residuals, being minimized by searching for the best estimation of $\vectorsym{\beta}$ is:
%
\begin{equation}
  \argmin{b} = \sum_{i=1}^n (y_i - x_i' b)^2 = (\vectorsym{y}-\matrixsym{X}b)^T (\vectorsym{y} - \matrixsym{X}b)
\end{equation}
%



%---------------------------------------------
%---------------------------------------------
\clearpage
\section{Online novelty detection on temporal sequences, by Ma and Perkins}
Paper: \cite{ma2003online}.
This method uses support vectors for regression (in contrast with of classification).
The regression function, using a kernel function $K(x_i, x_j)$ can be written as:
%
\begin{equation}
  f(\vectorsym{x}) = \sum_{i=1}^l \theta_i K(\vectorsym{x}_i, \vectorsym{x}) + b,
\end{equation}
%
where $\theta_i$ is a coefficient resulting from the Lagrange multipliers of the original minimization problem.
A small fraction these of coefficients are non-zero, and the corresponding samples $\vectorsym{x}_i$ are the \emph{support vectors}.
The regression function $f(\vectorsym{x})$ is non-linear when a non-linear kernel is chosen.

The regression function is used to created a model of past observations.
A matching function is constructed which determines the matching value $V(t_0)$ of a new observations with the constructed model.
This matching value is the residual of the regression function at $t_0$.

The algorithm determines \emph{(novel) events}, \emph{occurrences} and \emph{surprises}.
\emph{Novel events} are defined as a series of observations for which the confidence value over the number of suprises (out-of-model observations) is high enough.
Events thus have a length; they are constructed of a sub-series of observations.

The papers presents an alternative implementation in order to handle fixed-resource environments and thus induce an online algorithm.
After $W$ observations have been observed and used for the trained model, the oldest observation is disregarded before the newly obtained observation is incorporated.

\emph{Note:} the Support Vector approach in this paper is used to select the observations to use in the regression model.
This differs from one-class applications of support vector machines.
The same authors have also presented a paper which does use one-class construction using support vector machines: \cite{ma2003time}.