% !TEX root = ../main.tex
% Appendix C

\chapter{Summary of papers and principle formulas}\label{AppendixC}
\lhead{Appendix C. \emph{Paper summaries and formulas}}

\section{Change point detection in time series data using support vectors, by Camci}\label{sec:camci}
Paper: \cite{camci2010change}.
The main concept of this paper is to construct a hypersphere around the data and thereby generating a boundary.
A change point is detected when the radius of the hypersphere grows or shrinks significantly, or when a data point falls outside the boundary.

The main cost function being minimized:
%
\begin{equation}
  \minimize{r}
    {r^2 + C \sum_{i} \xi_i \norm{x}}
    {\norm{\vectorsym{x}_i - c}^2 \leq r^2 + \xi_i, \quad \xi_i \geq 0 \quad \forall i, \vectorsym{x}_i: i\text{th data point}}
\end{equation}
%
Where $r$ is the radius of a (hyper)circle with center $\vectorsym{c}$, $C$ is the penalty coefficient for every outlier and $\xi_i$ is the distance from the $i$th data point to hypersphere (also known as the slack variable).

The dual form by introducing the Lagrange multipliers ($\alpha_i, \alpha_i \geq 0$) and eliminating the slack variables $\xi$ is:
%
\begin{equation}
  \maximize{\vectorsym{\alpha}}
    {\sum_i \alpha_i (\vectorsym{x}_i \cdot \vectorsym{x}_i) - \sum_{i,j} \alpha_i \alpha_j (\vectorsym{x}_i \cdot \vectorsym{x}_j)}
    {0 \leq \alpha_i \leq C \quad\forall i, \quad \sum_i \alpha_i = 1}
\end{equation}
%
To allow for a non-linear relation between the data points and the data boundary, the inner product can be replaced by a (\eg Gaussian) kernel function: $K(\vectorsym{x}_i, \vectorsym{x}_j)$.





%---------------------------------------------
%---------------------------------------------
\clearpage
\section{Change-Point detection in time-series data by Direct Density-Ratio Estimation}
Paper: \cite{kawahara2009change}.

The density ratio $w(\matrixsym{Y})$ is modeled by a Gaussian kernel over sequences $\matrixsym{Y}$ of samples (sequence $\matrixsym{Y}_{te}(l)$ is the test sequence from the $l$th position on):
%
\begin{equation}
  \hat{w}(\matrixsym{Y}) = \sum_{l=1}^{n_\text{te}} \alpha_l K_\sigma(\matrixsym{Y}, \matrixsym{Y}_\text{te}(l)),
\end{equation}
%
where $\{\alpha_l\}_{l=1}^{n_\text{te}}$ are parameters to be learned from the data samples and $K_\sigma(\matrixsym{Y}, \matrixsym{Y}')$ is the Gaussian kernel function with mean $\matrixsym{Y}'$ and standard deviation $\sigma$.
The learned parameters minimize the Kullback-Leibler divergence from the sequence to the test sequence.
The maximization problem then becomes:
%
\begin{equation}
  \maximize{\{\alpha_l\}_{l=1}^{n_\text{te}}}
    {\sum_{i=1}^{n_\text{te}} \text{log} \left(\sum_{l=1}^{n_\text{te}} \alpha_l K_\sigma(\matrixsym{Y}_\text{te}(i), \matrixsym{Y}_\text{te}(l) ) \right)}
    {\frac{1}{n_\text{rf}} \sum_{i=1}^{n_\text{rf}} \sum_{l=1}^{n_\text{te}} \alpha_l K_\sigma(\matrixsym{Y}_\text{rf}(i), \matrixsym{Y}_\text{te}(l)) = 1, ~ \text{and} ~ \alpha_1, \dots ,\alpha_{n_\text{te}} \geq 1}
\end{equation}
%
With the estimated parameters the logarithm of the likelihood ratio between the test and reference interval can be calculated, which signals a change point if it is beyond a certain threshold $\mu$:
%
\begin{equation}
  S = \sum_{i=1}^{n_\text{te}} \text{ln}
    \frac
    {p_\text{te}(\matrixsym{Y}_\text{te}(i))}
    {p_\text{rf}(\matrixsym{Y}_\text{te}(i))}
\end{equation}







%---------------------------------------------
%---------------------------------------------
\clearpage
\section{Joint segmentation of multivariate time series with hidden process regression for human activity recognition, by Chamroukhi}\label{sec:appendix-C-joint-segmentation}
Paper: \cite{chamroukhi2013joint}.
This approach models the time series data by a parameterized regression, filtered with a \gls{hmm} to smooth out high frequency activity transitions.
With each observation $i$, generated by a $K$-state hidden process, an activity label $z_i$ (and thus sequence) is associated.
Observations follow a regression model:
%
\begin{equation}
  y_i = \beta_{z_i}^T \vectorsym{t}_i + \sigma_{z_i} \epsilon_i; \quad \epsilon_i \sim \mathcal{N}(0,1), \quad (i=1, \dots ,n)
\end{equation}
%
The observations get a label assigned by maximizing the logistic probability ($\pi_k$):
%
\begin{equation}
  \hat{z}_i = \argmaxold \limits_{1 \leq k \leq K} \pi_k (t_i; \hat{\vectorsym{w}}), \quad (i=1, \dots, n)
\end{equation}
%






%---------------------------------------------
%---------------------------------------------
\clearpage
\section{Support Vector Data Description, by Tax and Duin}
Paper: \cite{tax2004support}.
The paper proposes the \gls{svdd} method, analogous to the \gls{svc} of Vapnik \cite{vapnik1998statistical}, based on the separating hyper-plane of Sch{\"o}lkopf \etal \cite{scholkopf1999sv}.
Where \gls{svc} is ablo to distinguish data between two classes, \gls{svdd} obtains a closed boundary around the target class and can detect outliers.

Method and formulas very similair to description in Section \ref{sec:camci}.






%---------------------------------------------
%---------------------------------------------
\clearpage
\section{Support Vector Density Estimation, by Weston et al.}
Paper: \cite{weston1999support}.
Using the notation of \cite{weston1999support}, the distribution function of a density function $p(x)$ is represented as:
%
\begin{equation}
  F(x) = P(X \leq x) = \int_{- \infty}^x p(t)\,\mathrm{d}t
\end{equation}
%
To find the density the following linear equation need to be solved:
%
\begin{equation}
  \int_{-\infty}^\infty \theta(x-t)p(t)\,\mathrm{d}t = F(x)
\end{equation}
%
where
%
\[
  \theta(x) =
  \begin{cases}
    1,& x > 0 \\
    0,& otherwise
  \end{cases}
\]
%
In this problem the distribution function $F(x)$ is unknown and instead we are given the \gls{iid} data $x_1,\dots,x_l$ generated by $F$.

The empirical distribution function can now be constructed as:
%
\begin{equation}
  F_l(x) = \frac{1}{l} \sum_{i=1}^l \theta(x-x_i)
\end{equation}




%---------------------------------------------
%---------------------------------------------
\clearpage
\section{An online algorithm for segmenting time series, by Keogh et al.}\label{sec:appendix-C-online-keogh}
Paper: \cite{keogh2001online}.
This method approximates the signal with piecewise linear representation.
Change points are encountered at the time at which a new segment is used to represent the signal.
The method uses linear regression by taking the best fitting line in the least squares sense, since that minimizes the Euclidian distance which is used as a quality metric.
Linear interpolation is considered but since that always has a greater sum of squares error it is disregarded.

Linear regression assumes a relation from $n$ observations $\matrixsym{X}$ to the dependend variable $\vectorsym{y}$ using the parameter vector $\vectorsym{\beta}$:
%
\begin{equation}
  \vectorsym{y} = \matrixsym{X}\vectorsym{\beta} + \epsilon
\end{equation}
%
%
The error function, the sum of squared residuals, being minimized by searching for the best estimation of $\vectorsym{\beta}$ is:
%
\begin{equation}
  \argmin{b} = \sum_{i=1}^n (y_i - x_i' b)^2 = (\vectorsym{y}-\matrixsym{X}b)^T (\vectorsym{y} - \matrixsym{X}b)
\end{equation}
%



%---------------------------------------------
%---------------------------------------------
\clearpage
\section{Online novelty detection on temporal sequences, by Ma and Perkins}
Paper: \cite{ma2003online}.
This method uses support vectors for regression (in contrast with of classification).
The regression function, using a kernel function $K(x_i, x_j)$ can be written as:
%
\begin{equation}
  f(\vectorsym{x}) = \sum_{i=1}^l \theta_i K(\vectorsym{x}_i, \vectorsym{x}) + b,
\end{equation}
%
where $\theta_i$ is a coefficient resulting from the Lagrange multipliers of the original minimization problem.
A small fraction these of coefficients are non-zero, and the corresponding samples $\vectorsym{x}_i$ are the \emph{support vectors}.
The regression function $f(\vectorsym{x})$ is non-linear when a non-linear kernel is chosen.

The regression function is used to created a model of past observations.
A matching function is constructed which determines the matching value $V(t_0)$ of a new observations with the constructed model.
This matching value is the residual of the regression function at $t_0$.

The algorithm determines \emph{(novel) events}, \emph{occurrences} and \emph{surprises}.
\emph{Novel events} are defined as a series of observations for which the confidence value over the number of suprises (out-of-model observations) is high enough.
Events thus have a length; they are constructed of a sub-series of observations.

The papers presents an alternative implementation in order to handle fixed-resource environments and thus induce an online algorithm.
After $W$ observations have been observed and used for the trained model, the oldest observation is disregarded before the newly obtained observation is incorporated.

\emph{Note:} the Support Vector approach in this paper is used to select the observations to use in the regression model.
This differs from one-class applications of support vector machines.
The same authors have also presented a paper which does use one-class construction using support vector machines: \cite{ma2003time}.



%---------------------------------------------
%---------------------------------------------
\clearpage
\section{Time-series novelty detection using one-class support vector machines, by Ma and Perkins}
Paper: \cite{ma2003time}
This approach is very similar to the other paper of this author discussed in the preview section \cite{ma2003online}.
The difference is that this method does create a \gls{svm}-classifier to detect in and out of class examples, whilst the other uses the support vectors to construct a regression function.

The method constructs a hyper-plane which separates as many as possible data points in the feature space with the largest margin from the origin.
This is a different from (and more like the original \gls{svm}-proposal by Sch\"olkopf \cite{scholkopf1999support}) the one-class methodology by Tax which creates a boundary around the data \cite{tax2004support}.

The hyper-plane in feature space is represented as:
%
\begin{equation}\label{eq:hyper-plane-ma}
  f(\vectorsym{x}) = \matrixsym{W}^\transpose \Phi(\vectorsym{x}) - \rho,
\end{equation}
%
where $\Phi(\vectorsym{x})$ maps a vector $\vectorsym{x}$ from the input space $I$ to the (potentially infinite dimensional) feature space $F$.
$\matrixsym{W}$ and $\rho$ are determined by solving a quadratic optimization problem.
The dual formulation (using Lagrange multipliers $\alpha_i$) is:
%
\begin{equation}
  \matrixsym{W} = \sum_{i=1}^l \alpha_i \Phi(\vectorsym{x}_i),
\end{equation}
%
where $0 \leq \alpha_i \leq \frac{1}{\nu l}$.
The parameter $\nu \in (0,1)$ is set to trade-off the smoothness of $f(\vectorsym{x})$ and acts as a upper bound on the fraction of outliers over all the data examples in $\vectorsym{x}$ \cite{scholkopf1999support}.

Using the \emph{kernel trick} the inner product of two vectors in feature space $F$ can be replaced by a kernel function $K$, which is often the \gls{rbf}.
The equation of the hyper-plane (\ref{eq:hyper-plane-ma}) then becomes the following (non-linear) function:
%
\begin{equation}
  f(\vectorsym{x}) = \sum_{i=1}^l \alpha_i K(\vectorsym{x}_i, \vectorsym{x}) - \rho
\end{equation}
%

The form of the input vector $\vectorsym{x}$ is considered to be the \emph{(projected) phase space} representation of the original time series.
Just like \cite{kawahara2012sequential} which constructs \emph{sequences} of samples and in contrast with \cite{camci2010change}, each element of $\vectorsym{x}$ is a vector with the size of the \emph{embedding dimension} $E$ of the time series.
Thus, a time series $x(t)$ is converted to a set of vectors $T_E(N)$:
%
\begin{equation}
  T_E(N) = \{ \vectorsym{x}_E(t),\ t = E \cdots N\},
\end{equation}
%
where
%
\begin{equation}
  \vectorsym{x}_E(t) = [x(t - E + 1)\ x(t - E + 2)\ \cdots\ x(t)]
\end{equation}
%
If a point of this vector $\vectorsym{x}_E(t)$ is regarded (in the feature space $F$) as an outlier, all corresponding values in the original time series are also regarded as such.




%---------------------------------------------
%---------------------------------------------
\section{Least squares one-class support vector machine, by Choi}
Paper: \cite{choi2009least}.
The proposed method uses a \gls{svm} for a similarity/distance comparison for testing examples to training examples.
Instead of other methods, such as the standard one-class \gls{svm} by Sch\"olkopf \cite{scholkopf1999support} or the \gls{svdd} of Tax and Duin \cite{tax2004support}, it does not create a boundary for the training data.
Instead it uses a least-squared approach to construct a hyperplane to which most of the training examples lie close to.

The objective function to be minimized of the standard one-class method by Sch\"olkopf is formulated as:
%
\begin{equation}
  \minimize{\vectorsym{w}}
    {\frac{1}{2} \norm{\vectorsym{w}}^2 - \rho + C \sum_j \xi_j}
    {\vectorsym{w} \cdot \phi(\vectorsym{x}_j) \geq \rho - \xi_j \quad \text{and} \quad \xi_j \geq 0}
\end{equation}
%
where $\phi$ is a mapping to the feature space.

The least-squares one-class support vector machine has a small variation on the above formula, which results in the following minimization problem:
%
\begin{equation}\label{eq:minimize_least_squares_one_class_svm}
  \minimize{\vectorsym{w}}
    {\frac{1}{2} \norm{\vectorsym{w}}^2 - \rho + \frac{1}{2} C \sum_j \xi_j^2}
    {\vectorsym{w} \cdot \phi(\vectorsym{x}_j) = \rho - \xi_j}
\end{equation}
%
The slack variable (for which in the original formulation $\xi_j \geq 0$ should hold) now represents an error caused by a training example $\vectorsym{x}_j$ with relation to the hyperplane, \ie $\xi_j = \rho - \vectorsym{w} \cdot \phi(\vectorsym{x}_j)$.

In other words, the minimization problem (\ref{eq:minimize_least_squares_one_class_svm})) results in a hyperplane with maximal distance from the origin and for which the sum of the squares of errors $\xi_j^2$ are minimized.