% !TEX root = ../main.tex
% Appendix E

\chapter{Data structure quotes}

\cite{muller1997predicting}, ``Predicting Time Series with Support Vector Machines'', p.6. \\
``Our experiments show that SVR methods work particularly well if the data is sparse (i.e. we have little data in a high dimensional space). This is due to their good inherent regularization properties.''

\cite{mountrakis2011support}, ``Support vector machines in remote sensing: A review'', p.10. \\
``Most of the findings show that there is empirical evidence to support the theoretical formulation and motivation behind SVMs.
The most important characteristic is SVM’s ability to generalize well from a limited amount and/or quality of training data.
Compared to alternative methods such as backpropagation neural networks, SVMs can yield comparable accuracy using a much smaller training sample size.
This is in line with the ``support vector'' concept that relies only on a few data points to define the classifier's hyperplane.
This property has been exploited and has proved to be very useful in many of the applications we have seen thus far, mainly because acquisition of ground truth for remote sensing data is generally an expensive process.''\\
(And more)

\cite{camps2008kernel}, ``Kernel-based framework for multitemporal and multisource remote sensing data classification and change detection'', p.30. \\
``As core learners, the binary SVC and the one-class SVDD classiﬁer were used, and they were also benchmarked with neural networks in real scenarios.
In general, neural networks show inferior results compared to non-linear kernel classiﬁers, which is a direct consequence of their difﬁculties when working with very high dimensional input samples particularly important when stacking together other information sources such as contextual or multi-temporal''