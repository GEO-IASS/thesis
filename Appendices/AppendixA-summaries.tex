% !TEX root = ../main.tex
% Appendix A

\chapter{Summaries} % Main appendix title

\label{AppendixA} % For referencing this appendix elsewhere, use \ref{AppendixA}

\lhead{Appendix A. \emph{Summaries}} % This is for the header on each page - perhaps a shortened title

Please ignore this Appendix.
This appendix is for my own personal use.
It contains summaries of articles I have read.

%----------------------------------------------------------------------------------------
\section{Support Vector Machines}

\subsection{Machine learning: the art and science of algorithms that make sense of data}
Book by Peter Flach: \cite{flach2012machine}.
Mainly about chapter 7, ``Linear Models''.
Most important: section 7.3 - 7.5, about support vector machines and non-linearity.
\textbf{Some parts are direct text; do not use this text directly!}

\subsection{Linear models}
Models can be represented by their geometry of $d$ real-values features.
Data points are represented in the $d$-dimensional cartesian coordinate system/space $\mathcal{X} = \mathbb{R}^d$.
Geometric concepts such as lines and planes can be used for \emph{classification} and \emph{regression}.
An alternative approach is to use the distance between datapoints as a similarity measure, resulting from the geometrical representation.
Linear methods do not use that property, but rely on understanding of models in terms of lines and planes.

Linear models are of great interest in machine learning because of their simplicity.
A few manifestations of this simplicity are:
\begin{itemize}
  \item Linear models are \emph{parametric}, thus fixed small number of parameters that need to be learned from the data.
  \item Linear models are \emph{stable}, thus small variations in training data have small impact on the learned model. In logical models they can have large impact, because ``splitting rules'' in root have great impact.
  \item Due to relative few parameters, less likely to \emph{overfit} the training data.
\end{itemize}

The last two are summarized by saying that \emph{linear models have low variance but high bias}.
This is preferred with limited data and overfitting is to be avoided.

Linear models are well studied, in particular for the problem of linear regression.
This can be solved by the \emph{least-squares} method and classification as discussed in section \ref{least-squares}, the \emph{perceptron} as explained in section \ref{perceptron}.
Linear regression with the \emph{support vector machine} is handled in section \ref{svm-explained} and used for probability density estimation in section \ref{svm-pdf}.
The kernel trick used for learning non-linear models is explained in section \ref{non-linear}.

\subsection{Least-squares method}\label{least-squares}
The regression problem is to learn a function estimator $\hat{f}:\mathcal{X} \to \mathbb{R}$ from the examples $(x_i, f(x_i))$ where we assume $\mathcal{X} = \mathbb{R}^d$.
The difference between the actual and estimated function values are called \emph{residuals} $\epsilon_i = f(x_i) - \hat{f}(x_i)$.
The \emph{least-squares method} finds the estimation $\hat{f}$ by minimizing $\sum_{i=1}^{n} \epsilon_i^2$.
Univariate regressesion assumes a linear equation $y = a + b x$, with parameters $a$ and $b$ chosen such that the sum of squared residuals $\sum_{i=1}^{n} (y_i - (a + b x_i))^2$ is minimized.
Here the estimated parameter $\hat{a}$ is called the \emph{intercept} such that it goes through the (estimated) pooint $(\hat{x},\hat{y})$ and $\hat{b}$is the \emph{slope} which can be expressed by the (co)variances: $\hat{b} = \frac{\sigma_{xy}}{\sigma_{xx}}$.
In order to find the parameters, take the partial derivatives, set them to $0$ and solve for $a$ and $b$.

Although least-squares is sensitive to outliers, it works very well for such a simple method.
This can be explained as folows.
We can assume the underlying function is indeed linear but contanimated with random noise.
That means that our examples are actually $(x_i, f(x_i) + \epsilon_i)$ and $f(x) = ax + b$.
If we know $a$ and $b$ we can calculate what the residuals are, and by knowing $\sigma^2$ we can estimate \emph{the probability of observering the residuals}.
But since we don't know $a$ and $b$ we have to estimate them, by estimating the values for $a$ and $b$ that maximizes the probability of the residuals.
This is the \emph{maximum-likelihood estimate} (chapter 9 in the book).

The least-squares method can be used for a (binary) classifier, by encoding the target variable $y$ as classes by real numbers $-1$ (negative) and $1$ (positive).
It follows that $\matrixsym{X}^T\vectorsym(y) = P \vectorsym{\mu^+} - N \vectorsym{\mu^-}$, where $P$, $N$, $\vectorsym{\mu^+}$ and $\vectorsym{\mu^-}$ are the number of positive and negative examples, and the $d$-vectors containing each feature's mean values, resp.
The regression equation $y = \bar{y} + \hat{b}(x - \bar{x})$ can be used to obtain a decision boundary.
We need to determine the point $(x_0, y_0)$ such that $y_0$ is half-way between $y^+$ and $y^-$ (the positive and negative examples, i.e. $y_0 = 0$).

\subsection{Perceptron}\label{perceptron}
Labelled data is \emph{linearly separable} if the exists a linear boundary separating the classes.
The least-squares may find one, but it is not guaranteed.
Image a perfect linearly separatable data set.
Move all the positive points away from the negative, but one.
At one point the new boundary will exclude (misqualify) the one original positive outlier, due to the mean-statistics it relies on.
The \emph{perceptron} will guaranteed perform perfect separation when the data allows it to be.
It was originally proposed as a \emph{simple neural network}.
It works by iterating over the training set and modifying the weight vector for every misclassified example ($\vectorsym{w} \cdot \vectorsym{x}_i < t$ for positive examples $\vectorsym{x}_i$).
It uses a learning rate $\eta$, for a misclassified $y_i = \left\{-1,+1\right\}$: $\vectorsym{w}' = \vectorsym{w} + \eta y_i \vectorsym{x}_i$.
The algorithm can be made \emph{online} by processing a stream of data points and and updating the weight vector only when a new data point is misclassified.

When the algorithm is completed, every $y_i\vectorsym{x}_i$ is added $\alpha_i$ times to the weight vector (every time it was misclassified).
Thus, the weight vector can be expressed as: $\vectorsym{w} = \sum_{i=i}^n \alpha_i y_i \vectorsym{x}_i$.
In other words: the weight vector is a linear combination of the training instances.
The dual form of the algorithm learns the instance weights $\alpha_i$ rather than the features weights $\vectorsym{w}_i$.
An instance $\vectorsym{x}$ is then classified as $\hat{y} = sign(\sum_{i=1}^n \alpha_i y_i \vectorsym{x}_i \cdot \vectorsym{x})$.
This means that during the training only the pairwise dot-products of the data is needed; this results in the \emph{n}-by-\emph{n} Gram matrix $\matrixsym{G} = \matrixsym{X}\matrixsym{X}^T$.
This instance-based perspective will be further discussed in section \ref{svm-explained} about the support vector machine.

\subsection{Support Vector Machine}\label{svm-explained}
A training example can be expressed by its \emph{margin}: $c(x)\hat{s}(x)$, where $c(x)$ is $+1$ for positive and $-1$ for negative examples and $\hat{s}(x)$ is the score.
The score can be expressed as $\hat{s}(\vectorsym{x}) = \vectorsym{w} \cdot \vectorsym{x} - t$.
A true positive example $\vectorsym{x}_i$ has a margin $\vectorsym{w} \cdot \vectorsym{x}_i > 0$ and a true negative $\vectorsym{x}_j$ has $\vectorsym{w} \cdot \vectorsym{x}_j < 0$.
If $m^+$ and $m^-$ are the smallest positive and negative examples, then we want the sum of these to be as large as possible.
\emph{The training examples with these minimal values are closest to the decision boundary $t$ and are called the support vectors.}
The decision boundary is defined as a linear combination of the support vectors.
The margin is thus defined as $\frac{m}{\norm{\vectorsym{w}}}$.
Minimizing the margin (which is often set to $1$ and rescaling is allowed) yields to minimizing $\norm{\vectorsym{w}}$, or: $\frac{1}{2}\norm{\vectorsym{w}}^2$, restricted that none of the training points fall inside the margin.
This gives the following quadratic, constrained optimisation problem:

\begin{equation}
\begin{aligned}
  & \vectorsym{w}^*, t^* = \argmin\limits_{\vectorsym{w},t} \frac{1}{2} \norm{w}^2 & & \text{subject to } y_i(\vectorsym{w} \cdot \vectorsym{x}_i - t) \geq 1, 1 \leq i \leq n
\end{aligned}
\end{equation}

This equation can be transformed with the Lagrange multipliers by adding the constraints to the minimization part with multipliers $\alpha_i$.
Taking the partial derivative with respect to $t$ and setting it to $0$, we find that for the optimal solution (threshold) $t$ we have $\sum_{i=1}^{n} \alpha_i y_i = 0$.
When we take the partial derivative with respect to $w\vectorsym{w}$ we see that the Langrange multipliers define the weight vector as a linear combination of the training examples.
This partial derivative is $0$ for an optimal weight we get that $\vectorsym{w} = \sum_{i=1}^{n} \alpha_i y_i \vectorsym{x}_i$, \emph{which is the same expression as for the perceptrond derived in section \ref{perceptron}}.
By plugging $\vectorsym{w}$ and $t$ back into the Langrange equation, we can eliminate these and get the dual optimization problem entirely formulated in terms of the Lagrange multipliers:

\begin{equation}
  \Lambda(\alpha_1, \dots, \alpha_n) = - \frac{1}{2} \sum\limits_{i=i}^n \sum\limits_{j=1}^n \alpha_i \alpha_j y_i y_j \vectorsym{x}_i \cdot \vectorsym{x}_j + \sum\limits_{i=1}^n \alpha_i
\end{equation}

The dual problem maximizes this function under positivity constaints and one equality constraint:

\begin{equation}
\begin{aligned}
  \alpha_1^*, \dots, \alpha_n^* = \argmax\limits_{\alpha_1, \dots, \alpha_n} - \frac{1}{2} \sum\limits_{i=i}^n \sum\limits_{j=1}^n \alpha_i \alpha_j y_i y_j \vectorsym{x}_i \cdot \vectorsym{x}_j + \sum\limits_{i=1}^n \alpha_i \\
  \text{subject to } \alpha_i \geq 0, 1 \leq i \leq n \text{ and } \sum\limits_{i=1}^n \alpha_i y_i = 0
\end{aligned}
\end{equation}

This shows to important properties:
\begin{enumerate}
  \item Searching for the maximum-margin decision boundary is equivalent to searching for the support vectors; they are the training examples with non-zero Lagrange multipliers.
  \item The optimization problem is entirely defined by pairwise dot products between training instances: the entries of the Gram matrix.
\end{enumerate}

The second property enables powerful adaption for support vector machines to learn non-linear decision boundaries, as discussed in section \ref{non-linear}.

An other solution to non-linear separable data, that is when the constraints $\vectorsym{w} \cdot \vectorsym{x}_i - t \geq 1$ are not jointly satisfiable, is to add \emph{slack variables} $\xi_i$, one for each example.
This allows them to be in the margin, of even at the wrong side of the boundary -- known as boundary errors.
Thus, the constaints become $\vectorsym{w} \cdot \vectorsym{x}_i - t \geq 1 - \xi_i$.

``In summary, \emph{support vector machines are linear classifiers that construct the unique decision boundary that maximises the distance to the nearest training examples (the support vectors)}.
Training an SVM involves solving a large quadratic optimisation problem and is usually best left to a dedicated numerical solver.''

\subsection{Support Vector Machine Density Functions}\label{svm-pdf}
\subsection{Non-linear models}\label{non-linear}


%----------------------------------------------------------------------------------------