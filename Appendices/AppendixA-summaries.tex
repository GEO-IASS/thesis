% !TEX root = ../main.tex
% Appendix A

\chapter{Summaries} % Main appendix title

\label{AppendixA} % For referencing this appendix elsewhere, use \ref{AppendixA}

\lhead{Appendix A. \emph{Summaries}} % This is for the header on each page - perhaps a shortened title

\emph{Please ignore this Appendix.
This appendix is for my own personal use.
It contains summaries of articles I have read.
}
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
\section{Support Vector Machines}

\subsection{Machine learning: the art and science of algorithms that make sense of data}
Book by Peter Flach: \cite{flach2012machine}.
Mainly about chapter 7, ``Linear Models''.
Most important: section 7.3 - 7.5, about support vector machines and non-linearity.
\textbf{Some parts are direct text; do not use this text directly!}


%--------------------------------------------
\subsubsection{Linear models}
Models can be represented by their geometry of $d$ real-values features.
Data points are represented in the $d$-dimensional Cartesian coordinate system/space $\mathcal{X} = \mathbb{R}^d$.
Geometric concepts such as lines and planes can be used for \emph{classification} and \emph{regression}.
An alternative approach is to use the distance between data points as a similarity measure, resulting from the geometrical representation.
Linear methods do not use that property, but rely on understanding of models in terms of lines and planes.

Linear models are of great interest in machine learning because of their simplicity.
A few manifestations of this simplicity are:
\begin{itemize}
  \item Linear models are \emph{parametric}, thus fixed small number of parameters that need to be learned from the data.
  \item Linear models are \emph{stable}, thus small variations in training data have small impact on the learned model. In logical models they can have large impact, because ``splitting rules'' in root have great impact.
  \item Due to relative few parameters, less likely to \emph{overfit} the training data.
\end{itemize}

The last two are summarized by saying that \emph{linear models have low variance but high bias}.
This is preferred with limited data and overfitting is to be avoided.

Linear models are well studied, in particular for the problem of linear regression.
This can be solved by the \emph{least-squares} method and classification as discussed in section \ref{least-squares}, the \emph{perceptron} as explained in section \ref{perceptron}.
Linear regression with the \emph{support vector machine} is handled in section \ref{svm-explained} and used for probability density estimation in section \ref{linear-classifier-pdf}.
The kernel trick used for learning non-linear models is explained in section \ref{non-linear}.


%--------------------------------------------
\subsubsection{Least-squares method}\label{least-squares}
The regression problem is to learn a function estimator $\hat{f}:\mathcal{X} \to \mathbb{R}$ from the examples $(x_i, f(x_i))$ where we assume $\mathcal{X} = \mathbb{R}^d$.
The difference between the actual and estimated function values are called \emph{residuals} $\epsilon_i = f(x_i) - \hat{f}(x_i)$.
The \emph{least-squares method} finds the estimation $\hat{f}$ by minimizing $\sum_{i=1}^{n} \epsilon_i^2$.
Univariate regressesion assumes a linear equation $y = a + b x$, with parameters $a$ and $b$ chosen such that the sum of squared residuals $\sum_{i=1}^{n} (y_i - (a + b x_i))^2$ is minimized.
Here the estimated parameter $\hat{a}$ is called the \emph{intercept} such that it goes through the (estimated) pooint $(\hat{x},\hat{y})$ and $\hat{b}$is the \emph{slope} which can be expressed by the (co)variances: $\hat{b} = \frac{\sigma_{xy}}{\sigma_{xx}}$.
In order to find the parameters, take the partial derivatives, set them to $0$ and solve for $a$ and $b$.

Although least-squares is sensitive to outliers, it works very well for such a simple method.
This can be explained as follows.
We can assume the underlying function is indeed linear but contaminated with random noise.
That means that our examples are actually $(x_i, f(x_i) + \epsilon_i)$ and $f(x) = ax + b$.
If we know $a$ and $b$ we can calculate what the residuals are, and by knowing $\sigma^2$ we can estimate \emph{the probability of observing the residuals}.
But since we don't know $a$ and $b$ we have to estimate them, by estimating the values for $a$ and $b$ that maximizes the probability of the residuals.
This is the \emph{maximum-likelihood estimate} (chapter 9 in the book).

The least-squares method can be used for a (binary) classifier, by encoding the target variable $y$ as classes by real numbers $-1$ (negative) and $1$ (positive).
It follows that $\matrixsym{X}^T\vectorsym(y) = P \vectorsym{\mu^+} - N \vectorsym{\mu^-}$, where $P$, $N$, $\vectorsym{\mu^+}$ and $\vectorsym{\mu^-}$ are the number of positive and negative examples, and the $d$-vectors containing each feature's mean values, resp.
The regression equation $y = \bar{y} + \hat{b}(x - \bar{x})$ can be used to obtain a decision boundary.
We need to determine the point $(x_0, y_0)$ such that $y_0$ is half-way between $y^+$ and $y^-$ (the positive and negative examples, \ie $y_0 = 0$).


%--------------------------------------------
\subsubsection{Perceptron}\label{perceptron}
Labeled data is \emph{linearly separable} if the exists a linear boundary separating the classes.
The least-squares may find one, but it is not guaranteed.
Image a perfect linearly separable data set.
Move all the positive points away from the negative, but one.
At one point the new boundary will exclude (mis qualify) the one original positive outlier, due to the mean-statistics it relies on.
The \emph{perceptron} will guaranteed perform perfect separation when the data allows it to be.
It was originally proposed as a \emph{simple neural network}.
It works by iterating over the training set and modifying the weight vector for every misclassified example ($\vectorsym{w} \cdot \vectorsym{x}_i < t$ for positive examples $\vectorsym{x}_i$).
It uses a learning rate $\eta$, for a misclassified $y_i = \left\{-1,+1\right\}$: $\vectorsym{w}' = \vectorsym{w} + \eta y_i \vectorsym{x}_i$.
The algorithm can be made \emph{online} by processing a stream of data points and and updating the weight vector only when a new data point is misclassified.

When the algorithm is completed, every $y_i\vectorsym{x}_i$ is added $\alpha_i$ times to the weight vector (every time it was misclassified).
Thus, the weight vector can be expressed as: $\vectorsym{w} = \sum_{i=i}^n \alpha_i y_i \vectorsym{x}_i$.
In other words: the weight vector is a linear combination of the training instances.
The dual form of the algorithm learns the instance weights $\alpha_i$ rather than the features weights $\vectorsym{w}_i$.
An instance $\vectorsym{x}$ is then classified as $\hat{y} = sign(\sum_{i=1}^n \alpha_i y_i \vectorsym{x}_i \cdot \vectorsym{x})$.
This means that during the training only the pairwise dot-products of the data is needed; this results in the \emph{n}-by-\emph{n} Gram matrix $\matrixsym{G} = \matrixsym{X}\matrixsym{X}^T$.
This instance-based perspective will be further discussed in section \ref{svm-explained} about the support vector machine.


%--------------------------------------------
\subsubsection{Support Vector Machine}\label{svm-explained}
A training example can be expressed by its \emph{margin}: $c(x)\hat{s}(x)$, where $c(x)$ is $+1$ for positive and $-1$ for negative examples and $\hat{s}(x)$ is the score.
The score can be expressed as $\hat{s}(\vectorsym{x}) = \vectorsym{w} \cdot \vectorsym{x} - t$.
A true positive example $\vectorsym{x}_i$ has a margin $\vectorsym{w} \cdot \vectorsym{x}_i > 0$ and a true negative $\vectorsym{x}_j$ has $\vectorsym{w} \cdot \vectorsym{x}_j < 0$.
If $m^+$ and $m^-$ are the smallest positive and negative examples, then we want the sum of these to be as large as possible.
\emph{The training examples with these minimal values are closest to the decision boundary $t$ and are called the support vectors.}
The decision boundary is defined as a linear combination of the support vectors.
The margin is thus defined as $\frac{m}{\norm{\vectorsym{w}}}$.
Minimizing the margin (which is often set to $1$ and rescaling is allowed) yields to minimizing $\norm{\vectorsym{w}}$, or: $\frac{1}{2}\norm{\vectorsym{w}}^2$, restricted that none of the training points fall inside the margin.
This gives the following quadratic, constrained optimization problem:

\begin{equation}
\begin{aligned}
  \argmin{\vectorsym{w},t} = \frac{1}{2} \norm{w}^2 & & \text{subject to } y_i(\vectorsym{w} \cdot \vectorsym{x}_i - t) \geq 1, 1 \leq i \leq n
\end{aligned}
\end{equation}

This equation can be transformed with the Lagrange multipliers by adding the constraints to the minimization part with multipliers $\alpha_i$.
Taking the partial derivative with respect to $t$ and setting it to $0$, we find that for the optimal solution (threshold) $t$ we have $\sum_{i=1}^{n} \alpha_i y_i = 0$.
When we take the partial derivative with respect to $w\vectorsym{w}$ we see that the Lagrange multipliers define the weight vector as a linear combination of the training examples.
This partial derivative is $0$ for an optimal weight we get that $\vectorsym{w} = \sum_{i=1}^{n} \alpha_i y_i \vectorsym{x}_i$, \emph{which is the same expression as for the perceptron derived in section \ref{perceptron}}.
By plugging $\vectorsym{w}$ and $t$ back into the Lagrange equation, we can eliminate these and get the dual optimization problem entirely formulated in terms of the Lagrange multipliers:

\begin{equation}
  \Lambda(\alpha_1, \dots, \alpha_n) = - \frac{1}{2} \sum\limits_{i=i}^n \sum\limits_{j=1}^n \alpha_i \alpha_j y_i y_j \vectorsym{x}_i \cdot \vectorsym{x}_j + \sum\limits_{i=1}^n \alpha_i
\end{equation}

The dual problem maximizes this function under positivity constraints and one equality constraint:
*** TODO: fix equation (now commented) ***
\begin{equation}
\begin{aligned}
  % \alpha_1^*, \dots, \alpha_n^* = \argmax\limits_{\alpha_1, \dots, \alpha_n} - \frac{1}{2} \sum\limits_{i=i}^n \sum\limits_{j=1}^n \alpha_i \alpha_j y_i y_j \vectorsym{x}_i \cdot \vectorsym{x}_j + \sum\limits_{i=1}^n \alpha_i \\
  \text{subject to } \alpha_i \geq 0, 1 \leq i \leq n \text{ and } \sum\limits_{i=1}^n \alpha_i y_i = 0
\end{aligned}
\end{equation}

This shows to important properties:
\begin{enumerate}
  \item Searching for the maximum-margin decision boundary is equivalent to searching for the support vectors; they are the training examples with non-zero Lagrange multipliers.
  \item The optimization problem is entirely defined by pairwise dot products between training instances: the entries of the Gram matrix.
\end{enumerate}

The second property enables powerful adaption for support vector machines to learn non-linear decision boundaries, as discussed in section \ref{non-linear}.

An other solution to non-linear separable data, that is when the constraints $\vectorsym{w} \cdot \vectorsym{x}_i - t \geq 1$ are not jointly satisfiable, is to add \emph{slack variables} $\xi_i$, one for each example.
This allows them to be in the margin, of even at the wrong side of the boundary -- known as boundary errors.
Thus, the constraints become $\vectorsym{w} \cdot \vectorsym{x}_i - t \geq 1 - \xi_i$.

``In summary, \emph{support vector machines are linear classifiers that construct the unique decision boundary that maximizes the distance to the nearest training examples (the support vectors)}.
Training an SVM involves solving a large quadratic optimization problem and is usually best left to a dedicated numerical solver.''

%--------------------------------------------
\subsubsection{Density Functions from linear classifiers}\label{linear-classifier-pdf}
The score of an data point can be used to obtain the signed distance of $\vectorsym{x}_i$ to the decision boundary:
\begin{equation}
  d(\vectorsym{x}_i) = \frac{\hat{s}(\vectorsym{x}_i)}{\norm{w}} = \frac{\vectorsym{w} \cdot \vectorsym{x}_i - t}{\norm{w}} = \vectorsym{w}' \cdot \vectorsym{x}_i - t'
\end{equation}
where $\vectorsym{w}' = \vectorsym{w} / \norm{\vectorsym{w}}$ rescaled to unit length and $t' = t / \norm{w}$ corresponds to the rescaled intercept.
this geometric interpretation of the scores enables them to turn into probabilities.
Let $\bar{d}^+ = \vectorsym{w} \cdot \vectorsym{\mu}^+ - t$ denote the mean distance of the positive examples to the boundary, where $\vectorsym{\mu}^+$ is the mean of positive examples (in the grid) and $\vectorsym{w}$ is unit length.
We can assume that the distance of the examples is normally distributed around the mean (which give a bell curve when plotted).

If we obtain a new point $\vectorsym{x}$ we can get the class by $sign(d(\vectorsym{x}))$.
We would like, instead, to get the probability (using Bayes' rule)
\begin{equation}
  \hat{p}(\vectorsym{x}) = P(+|d(\vectorsym{x}) = \frac{P(d(\vectorsym{x})|+)P(+)}{P(d(\vectorsym{x})|+)P(+) + P(d(\vectorsym{x})|-)P(-)} = \frac{LR}{LR + 1/clr}
\end{equation}
where $LR$ is the likelihood ratio obtained from the normal score distributions, and $clr$ is the class ratio.
With some rewriting we can convert $d$ into a probability by means of the mapping $d \mapsto \frac{exp(d)}{exp(d) + 1}$, which is the \emph{logistic function}.
The logarithm of the likelihood ratio is linear in $\vectorsym{x}$ and such models are called \emph{log-linear models}.
This logistic calibration procedure can change the location of the decision boundary but not ts direction.
There may be an alternative weight vector with a different direction that assign a higher likelihood to the data.
Finding that maximum-likelihood linear classifier using the logistic model is called \emph{logistic regression}.

%--------------------------------------------
\subsubsection{Non-linear models}\label{non-linear}
Linear methods such as least-squares for regression can be used for binary classification, yielding in the basic linear classifier.
The (heuristic) perceptron guaranteers to classify correctly linear separable data points.
Support vector machines find the unique decision boundary with maximum margin and can be adapted to non-linear separable data.
These methods can be adjusted to learn non-linear boundaries.
The main idea is to transform the data from the \emph{input space} non-linearly to a \emph{feature space} (which can, but does need to be in a higher dimension) in which linear classification can be applied.
The mapping back from the feature space to the input space is often non-trivial (e.g. mapping $(x,y)$ to feature space by $(x^2, y^2)$, yields in four coordinates when transformed back to the input space).

\emph{The remarkable thing is that often the feature space does not have to be explicitly constructed, as we can perform all necessary operations in input space.}
For instance; the perceptron algorithm mainly depends on the dot product of $\vectorsym{x}_i \cdot \vectorsym{x}_j$.
Assuming $\vectorsym{x}_i = (x_i, y_i)$ and $\vectorsym{x}_j = (x_j, y_j)$, the dot product can be written as $\vectorsym{x}_i \cdot \vectorsym{x}_j = x_i x_j + y_i y_j$.
The instances in quadratic feature space are $(x_i^2, y_i^2)$ and $(x_j^2, y_j^2)$ and their dot product is $(x_i^2, y_i^2) \cdot (x_j^2, y_j^2) = x_i^2 x_j^2 + y_i^2 y_j^2$.
This is almost equal to $(\vectorsym{x}_i \cdot \vectorsym{x}_j)^2 = (x_i x_j)^2 + (y_i y_j^2 + 2 x_i x_j y_i y_j$, but not quite because of the third term.
We can make the equations equal by \emph{extending the feature space} (to a higher dimension) with a third feature $\sqrt{2xy}$, so the feature space is $\phi(\vectorsym{x}_i) = (x_i^2, y_i^2, \sqrt{2x_i y_i})$.

If we define $\kappa(\vectorsym{x}_i, \vectorsym{x}_j) = (\vectorsym{x}_i, \vectorsym{x}_j)^2$ and replace $\vectorsym{x}_i \cdot \vectorsym{x}_j$ with $\kappa(\vectorsym{x}_i, \vectorsym{x}_j)$ in the (perceptron) algorithm, we obtain the \emph{kernel perceptron} with the degree $p = 2$.
We are not restricted to polynomial kernels; an often used kernel is the \emph{Gaussian kernel}, defined as:
\begin{equation}
  \kappa(\vectorsym{x}_i, \vectorsym{x}_j) = exp(\dfrac{-\norm{\vectorsym{x}_i - \vectorsym{x}_j}^2}{2\sigma^2})
\end{equation}
where $\sigma$ is known as the \emph{bandwidth} parameter. We can think of the Gaussian kernel as imposing a Gaussian (\ie, multivariate normal) surface on each support vector in instance space, so that the boundary is defined in therms of those Gaussian surfaces.
Kernel methods are best known in combination with support vector machines.
Notice that the soft margin optimization problem is defined in terms of dot product between training examples, and thus the `kernel trick' can be applied.
Note that the decision boundary learn with a non-linear kernel cannot be represented by a simple weight vector in input space.
Thus, to classify a new example $\vectorsym{x}$ we need to evaluate $y_i \sum_{j=1}^n \alpha_j y_j \kappa(\vectorsym(x), \vectorsym{x}_j)$ (the Gram matrix?) involving all training examples, or at least all with non-zero multipliers $a_j$ (the support vectors).



%----------------------------------------------------------------------------------------

\subsection{Change Point Detection In Time Series Data Using Support Vectors}
Paper by Fatih Camci \cite{camci2010change}
About segmentation with SVMs.
Will be main material for section \ref{sec:svm_in_change_detection} about SVMs.

\subsubsection{Introduction}
Interprets change detection as finding the transition points from one underlying time series generation model to another.
The change point is mostly represented in a sudden change in mean or variance.
Existing models detect changes in mean and increase in variance, \emph{but fail to recognize decrease in variance}.
Many methods require some model (like Auto-Regressive [AR]) to fit the time series in order to eliminate the noise.
Thus, the effectiveness of the method is tied to the fitness degree of the model to the time series data.
These two problems (lack of variance decrease detection and model-bound fitness degree) leads to this work; Support Vector based Change Point Detection targeting changes in variance and/or mean without any assumption of model fitting of data distribution.
This method \emph{does not use a time series model} for fitting and targets \emph{both increase and decrease} in mean and variance.

\subsubsection{Related work}
Change Point Detection (CPD) can be categorized in posterior (off-line) and sequential (on-line).
Sequential receive data sequentially and analyze previously obtained data to detect the possible change in current time.
This method is based on sequential analysis and focuses on change on mean and variance in time domain.
Other methods generally suffer from:
\begin{itemize}
  \item Inability / inefficiency in detecting variance decrease.
  \item Assumptions about the statistical distribution of the data, obtained as error of fitting the (AR) model.
  \item Necessity of training the model with possible changes.
\end{itemize}

\subsubsection{Support vector based one-class classification}
Although SVM was originally designed for two-class classification, it has been successfully applied to multi-class and one-class classification.
SVM-based one-class classification gives the minimum volume closed spherical boundary around the data, represented by center $c$ and radius $r$.
It minimizes $r^2$ (representing structural error), and uses a penalty coefficient $C$ for each outlier with distance $\xi_i$ from the hyper-sphere boundary:
\begin{equation}
\begin{aligned}
  \text{Min } r^2 + C \sum_{i} \xi_i \\
  \text{Subject to : } \norm{\vectorsym{x}_i - c}^2 \leq r^2 + \xi_i & & \xi_i \geq 0, & & \forall i, \vectorsym{x}_i: i\text{th data point}
\end{aligned}
\end{equation}
This quadratic optimization problem can be transformed to its dual form by introduction Lagrange multipliers $\alpha_i$.
If, for a data point, the multiplier $\alpha_i = 0$, then that point is inside the sphere.
When it is $0 < \alpha_i < C$, then it is on the boundary.
Data points for which the multiplier is $\alpha_i = C$ are located outside the sphere (and are penalized).
The dual form is:
\begin{equation}
\begin{aligned}
  \text{Max } \sum_i \alpha_i (\vectorsym{x}_i \cdot \vectorsym{x}_i) - \sum_{i,j} \alpha_i \alpha_j (\vectorsym{x}_i \cdot \vectorsym{x}_j) \\
  \text{Subject to : } 0 \leq \alpha_i \leq C & & \forall i, & & \sum_i \alpha_i = 1
\end{aligned}
\end{equation}
Note that only dot-products of the data points $\vectorsym{x}$ appear.
In order to transform the data points to a higher dimension, to create a good representational hyper-sphere, kernels replace the dot products without compromising computational complexity.
The problem then becomes:
\begin{equation}
  \text{Max } \sum_i \alpha_i K(\vectorsym{x}_i, \vectorsym{x}_i) - \sum_{i,j} \alpha_i \alpha_j K(\vectorsym{x}_i, \vectorsym{x}_j)
\end{equation}
It has been shown that Gaussian kernels offer better performance for one-class classification the others.
The optimization of the \emph{scale parameter} has led to several implementations.
As can been seen, there are no assumptions about the data distribution or independency made.

\subsubsection{Problem formulation}
[Not summarized here, useful for e.g. section \ref{statistical-framework}]

\subsubsection{SVCPD: The algorithm}
Instead of using statistical properties of the data, for each window of size $w$ a hyper-sphere is constructed without increasing computational complexity due to the \emph{kernel trick}.
The window size is related to the sensitivity of the method to change; small windows are sensitive with high false alarm rate whilst large windows are slow to detect change and have low alarm rates.
The algorithm is listed in table \ref{tab:alg-svcdp}.
Note that SVCPD can be applied directly to multidimensional data, whilst many other methods can only be applied to one-dimensional data.

\begin{center}\begin{table}
\begin{tabular}{ l p{12cm} }
  \hline
  Step & Action \\
  \hline
  1 & Start with $n$ observations and construct hyper-sphere \\
  2 & Add next observation $x_t$ and drop first one \\
  3 & Identify new hyper-sphere and its \emph{approximate radius} \\
  4 & if $x_t$ is outside hyper-sphere, mark $t$ as change points and continue from step 2 \\
  5 & calculate radius average of last $w$ hyper-planes \\
  6 & calculate radius ratio $\hbar$. If lower than ${th}_{low}$ or greater than ${th}_{high}$ then mark $t$ as change point \\
  7 & continue from step 2 \\
  \hline
\end{tabular}
\caption{Support Vector machine based Change Point Detection algorithm}
\label{tab:alg-svcdp}
\end{table}\end{center}





%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
\section{CUSUM for variance}

\subsection{Use of Cumulative Sums of Squares for Retrospective Detection of Changes of Variance}
Carla Inclan and George C. Tiao \cite{inclan1994use}, 1944, 162 refs.


%--------------------------------------------
\subsubsection{Introduction}
This paper is about reflective detection of multiple changes of variance in a sequence of independent observations.
This is a statistical method, which differs from others (in that field) such as Bayesian method (Bayes ratio, posterior odds), maximum likelihood methods and (autoregressive) models.
This approach uses the centered version of cumulative sums of squares to search for change points systematically and iteratively (and reflective).


%--------------------------------------------
\subsubsection{Centered Cumulative Sum of Squares}
The cumulative sum of squares is often used for change detection in the mean.
It is defined as $C_k = \sum_{i=1}^k \alpha_t^2$ for a series of uncorrelated random variables $\{\alpha_t\}$ with mean $0$ and variance $\sigma_t^2, t = 1, 2, \dots, T$.
The centered (and normalized) cumulative sum of squares is:
\begin{equation}
  \begin{aligned}
  D_k = \frac{C_k}{C_T} - \frac{k}{T}, & & k = 1, \dots, T, & & \text{with } D_0 = D_T = 0
  \end{aligned}
\end{equation}
For homogeneous variance the plot of $D_k$ against $k$ (the first $k$ elements of the series) will oscillate around $0$.
When a sudden change in variance occurs, the pattern of the plot of $D_k$ will break out some specified boundaries with high probability.
For $C_k$ it holds that, under homogeneous variance, the plot will be a straight line with slope $\sigma^2$.
With one of more change points the plot appears as a line of several straight pieces.
The plot of $D_k$ creates a peak for a smaller and a trough for a larger variance, is visually more clear and breaks out a predefined value.
The search for a change point is variance is than to find $k^* = \text{max}_k \abs{D_k}$.
If the value of $D_k$ at $k^*$ exceeds a predefined value (e.g. $D_{0.5}^* = 1.358$, for $\sqrt{T / 2D_k}$ because of the Brownian bridge property), that value of $k^*$ will be an estimate for a change point.

There is a relation between $D_k$ and the $F$ statistic, which is used for testing equality of variances between two independent samples.
For a fixed $k$, $D_k(F)$ is a monotone function of $F$ (it depends only on $k$ through $k/T$).
An important distinction: the $F$ statistic is \emph{used with known $k$}, whereas we are looking for $\text{max}_k \abs{D_k}$ to determine the location of the change point.

When we assume that $\{\alpha_t\}$ is normally distributed with mean $0$ and variances $\sigma_t^2$, then we can obtain the \emph{likelihood ratio} for testing the hypothesis of one change against the hypothesis of no change in the variance.
When maximizing the likelihood estimator for a location $\kappa$, we can find the log-likelihood ratio $LR_{0,1}$.
Although $LR_{0,1}$ and $\text{max}_k \abs{D_k}$ are related, they are not the same.
The latter puts more weight near the middle of the series is thus biased toward $T/2$.

The (expected) value of $D_k$ given a change in variance differs in the context.
If a smaller variance corresponds to the smaller portion of the series, then it will be harden to find the change point using $D_k$.
There is a masking effect when there are multiple change points in the series; the order of small, medium and large variances result in the value of $D_k$.
The iterative algorithm presented in this paper in section \ref{subsec:icss} is designed to lessen the masking effect.

%--------------------------------------------
\subsubsection{Multiple changes: Iterated cumulative sums of squares}\label{subsec:icss}
In case of a single change point the $D_k$ method would succeed.
But we are interested in multiple change points of variance, and thus the usefulness of the $D_k$ reduces due to the masking effect.
A solution is to iteratively applying the method and dividing the series at each possible change point.
The algorithm is presented in table \ref{tab:alg-icss}.
It is the third steps which reduces the masking effect and helps to ``fine tune'' the algorithm by (re)moving the potential change points by checking each location given the adjacent ones.

\begin{center}\begin{table}
\begin{tabular}{ l p{12cm} }
  \hline
  Step & Action \\
  \hline
  0 & Let $t_1 = 1$ \\
  1 & Calculate $D_k(\alpha[t_1 : T])$. Let $k^*(\alpha[t_1 : T])$ be the point at which $\text{max}_k \abs{D_k(\alpha[t_1 : T])}$. Let $D^*$ be the asymptotically critical value and $M$ the max value in the series segment (?). If $M > D^*$ then consider $k^*$ to be a change point. Else, there is no change point and the algorithm stops. \\
  2a & Repeat for the first part (up to the change point), until no more change points are found. \\
  2b & Repeat for the second part (from the change point forward), until no more change points are found. \\
  3 & When two or more change points are found; check for each $\alpha[j-1 : j+1]$ if there is indeed a change point ($j$). Repeat until the number of change points does not change and each new found change point is ``close'' enough to previous. \\
  \hline
\end{tabular}
\caption{Iterated Cumulative Sums of Squares Algorithm}
\label{tab:alg-icss}
\end{table}\end{center}

\subsubsection{Results}
When the \gls{icss} algorithm was applied to stock data, it resulted in comparable results as the maximum likelihood estimates and Bayesian analysis.
The performance (CPU-time and correct observations with artificial data) of \gls{icss} outperforms the other two.
The heavy computational burden of posterior odds can be partially alleviated by the maximum log-likelihood method.
The \gls{icss} algorithm avoids calculating a function at all possible locations of change points due the iterative manner.




%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
\section{Density Ratio Estimation}

\subsection{Change-Point Detection in Time-Series Data by Direct Density-Ratio Estimation}
Kawahara and Sugiyama \cite{kawahara2009change}, 2009, 45 refs.

``This paper provides a change-point detection algorithm based on direct density-ratio estimation that can be computed very efficiently in an online manner''.

%--------------------------------------------
\subsubsection{Introduction}
The problem of change-point detection is well studied over the last decades in the field of statistics.
A common statistical formulation of change-point detection is to consider the probability distributions over past and present data intervals, and regard the target time as a change-point if the two distributions are significantly different.
Some approaches (such as \gls{cusum} and GLR) make use of the \emph{log-likelihood ratio}, and are extensively explored in the data mining community.
Many approaches (novelty detection, maximum-likelihood ratio, learning of autoregressive models, subspace identification) rely on pre-specified parametric models (probability density models, autoregressive models, state-space models).
That makes it less applicable to real-life problems.
There have been some non-parametric density estimation approaches proposed, but that is known to be a hard problem.
The key idea of this paper to directly estimate the \emph{ratio} of the probability densities (also known as \emph{importance}).
The \gls{kliep} is an example, but it is a batch algorithm.
This paper introduces an online version of the \gls{kliep} algorithm and develops a flexible and computationally efficient change-point detection method.
This method is equipped with a natural cross validation procedure and thus the value of tuning parameters can be objectively determined.

\subsubsection{Problem formulation and Basic Approach}
Let $\vectorsym{y}(t) (\in \mathbb{R}^d)$ be a $d$-dimensional time series sample at time $t$.
The task is to detect whether there exists a change point between two consecutive time intervals, called the \emph{reference} and \emph{test} intervals.
The conventional algorithms consider the likelihood ratio over samples from the two intervals.
Since time-series samples generally are not independent over time it is hard to deal with them directly.
To overcome this difficulty, we consider \emph{sequences} of samples in the intervals: $\matrixsym{Y}(t)(\in \mathbb{R}^{dk})$ is the forward subsequence of length $k$ at time $t$.
This is a common practice in subspace identification since it takes implicitly time correlation into consideration.
The algorithm in the paper is based on the logarithm of the likelihood ration of the \emph{sequence sample} $\matrixsym{Y}$:
\begin{equation}
  s(\matrixsym{Y}) = \text{ln}\frac{p_\text{te}(\matrixsym{Y})}{p_\text{rf}(\matrixsym{Y})}
\end{equation}
where $p_{\text{te}}(\matrixsym{Y})$ and $p_{\text{rf}}(\matrixsym{Y})$ are the probability density functions of the reference and test sequence samples.
Let $t_{\text{rf}}$ and $t_{\text{te}}$ be the starting points of the reference and test intervals.
Decide if there is a change-point between the reference and test interval by monitoring the logarithm of the likelihood ratio:
\begin{equation}
  S = \sum_{i=1}^{n_\text{te}} \text{ln} \frac{p_{\text{te}}(\matrixsym{Y}_\text{te}(i))}{p_\text{rf}(\matrixsym{Y}_\text{te}(i))}
\end{equation}
If, for a predefined $\mu > 0$, it holds that $S > \mu$ then a change occurs.
The remaining question is how to calculate the density ratio, because it is unknown and we need to estimate it from examples.
The naive approach would be to first estimate the densities for the reference and test interval separately and then take the ratio.
This approach via non-parametric density estimation may not be effective --- directly estimating the density ratio without estimating the densities would be more promising.

The direct estimation of the density ratio is based on the Kullback-Leibler Importance Estimation Procedure.
Let us model the density ratio $w(\matrixsym{Y})$ by a non-parametric Gaussian kernel model:
\begin{equation}
  \hat{w}(\matrixsym{Y}) = \sum_{l=1}^{n_\text{te}} \alpha_l K_\sigma(\matrixsym{Y}, \matrixsym{Y}_\text{te}(l)),
\end{equation}
where $\{\alpha_l\}_{l=1}^{n_\text{te}}$ are parameters to be learned from the data samples and $K_\sigma(\matrixsym{Y}, \matrixsym{Y}')$ is the Gaussian kernel function with mean $\matrixsym{Y}'$ and standard deviation $\sigma$.

\subsubsection{Online Algorithm}
...


%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
\section{Outlier Detection methods}
\subsection{A Survey of Outlier Detection Methodologies}
Hodge and Austin, \cite{hodge2004survey}, 2004, 734 refs.

\subsection{Summary}
This survey takes a look at different methodologies that perform outlier detection.
It gives two definitions of outliers, one of which relates to the problem statement in this thesis (taken from \cite{barnett1994outliers}):
\begin{quote}An observation (or subset of observations) which appears to be inconsistent with the remainder of that set of data.\end{quote}

The survey introduces three fundamental approaches to the problem of outlier detection:
\begin{description}
  \item[Type 1] - no prior knowledge about the data; analogous to \emph{unsupervised clustering}. Flags the remote points as outliers; mainly batch-processing systems.
  \item[Type 2] - model (and requires) both normal and abnormal data; analogous to \emph{supervised classification}. Is able to process new data online.
  \item[Type 3] - Model only normal data (and in very few cases abnormal). Generally referred to as \emph{novelty detection}, analogous to \emph{semi-supervised recognition or detection}. It only requires pre-classified normal data. It aims to define a boundary of normality.
\end{description}

The type of approaches of interest in this research is of \textbf{type 3}.
It is characterized by the ability to recognize new data as normal when it lies within the constructed boundary and as a novelty otherwise.
This ability removes the need of sample-abnormality data, which may be hard (or costly) to produce.
The method of Tax \etal \cite{tax1999supportdata} is stated to be of \textbf{type 2}, and one can argue that other methods of Tax \etal \cite{tax2001one,tax2004support} are by their one-class classification instances of \textbf{type 3} methods.

The survey states that density-based methods estimate the density distribution of the training data.
Outliers are identified as data points lying in a low-density region.