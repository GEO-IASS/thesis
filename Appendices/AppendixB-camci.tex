% !TEX root = ../main.tex
% Appendix B

\chapter{Session with Anne 24-06-2013 - Paper Camci analysis} % Main appendix title

\label{AppendixB} % For referencing this appendix elsewhere, use \ref{AppendixA}

\lhead{Appendix B. \emph{Paper Camci}} % This is for the header on each page - perhaps a shortened title

\emph{Please ignore this Appendix.
This appendix is for my own personal use.
This chapter will look at the paper of Camci \cite{camci2010change} (``Change point detection in time series using support vectors'') and will answer many question that the paper leaves open.
The goal is to make a better justification for the used techniques and made assumptions.
}


%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
\section{Density estimation / Data description / Vapnik's principle}
Following Vapnik's principle, one should ``When solving a problem of interest, do not solve a more general problem as an intermediate step.'' \cite{vapnik1998statistical} when a limited amount of data is available.
For the problem of change detection we are only interested in some characteristics of the data.
Solving the complete density estimation might require more data than actually needed when the requested characteristic is a closed boundary around the data.

In \cite{liu2013change} it is stated that the \emph{Support Vector Machine} by Cortes and Vapnik \cite{cortes1995support} is representative example of this principle.
Instead of estimating the more general data generating probability distributions, it only learns a decision boundary to differentiate between the two distributions.

The proposed method \gls{svdd} of Tax and Duin \cite{tax1999support, tax2004support} models the boundary of data under consideration.
Thereby it characterizes a data set and can be used to detect novel data or outliers.
The performance is compared to methods which model the distribution's density instead, using Receiver-Operating Characteristic (ROC) curves and false negative rates.
The compared methods are:
\begin{inparaenum}[\itshape (1)]
  \item normal density which estimates the mean and covariance matrix;
  \item the Parzen density where the width of the Parzen kernel is estimated;
  \item a Gaussian Mixture Model optimized using EM; and
  \item the Nearest-Neighbor Method which compares the local density of an object with the density of the nearest neighbor in the target set (and is thus, just as SVDD, a boundary-based method).
\end{inparaenum}
The results show that when the problem formulation is to characterize an area in a feature space (and not the complete density distribution) SVDD gives a good data description.

The study of Tax on one-class classifiers \cite{tax2001one} further compares density methods, boundary methods (amongst which SVDD) and reconstruction methods.
One promesing result for SVDD is that it performs well on read-world data, for which generalization is needed.

The method of Camci \cite{camci2010change} uses a Support Vector based method to find change points in the data.
It does not explicitly create a density estimation, but instead relies on the spherical boundary and uses its ability to detect novel data or outliers to detect change points in time series data.

*** \emph{Thus methods of data descriptions should be compared (which are more general) and not only density estimation?} ***

The paper of Yin \etal \cite{yin2008sensor} makes a distinction between similarity based (using a defined distance measure) and model based (which characterize the data using predictive models) approach.
The one-class SVM is used in the model-approach to filter out normal activities in order to detect abnormality behavior.


%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
\section{Change point definition}
The method of Camci \cite{camci2010change} regards a change point as the moment in time that the underlying stochastic process has changed, say from $p^1$ to $p^2$.
It assumes that each of these stochastic processes is modeled following a Gaussian distribution, such that a change can occur in the value of the mean and/or the variance; $p^1 \sim N(\mu_1, \sigma_1^2)$.
The CUSUM-based method of \cite{inclan1994use} also regards each semgent as a Gaussian distribution.

The method if Kawahara \etal \cite{kawahara2009change} is based on the log likelihood ratio of test samples, and the method by Liu \etal \cite{liu2013change} uses a comparable dissimilarity measure using the KLIEP algorithm.

The method of Chamroukhi \etal \cite{chamroukhi2013joint} is based on a Hidden Markov Model and logistic regression.
It assumes a $K$-state hidden process with a (hidden) state sequence, each state providing the parameters (amongst which the order) for a polynomial.
The order of the model segment is determined by model selecting, often using the Bayesian Information Criterion (BIC) or the similair Akaike Information Criterion (AIC) \cite{akaike1974new}, as in \cite{he2008activity}.

*** REVIEW zoeken: change points in time series ***

Periodicity/consecutive data vs unique/irregular data?

Change in model parameters (mean/variance, of linear/non-linear)? --> Model selection?

Definition of continuity --> windows the domain and problem.
Will result in definition of dis-continuity --> this is the goal to find
Relation with double differentiation.



%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
\section{Data and model}
Why assume (model of) data is Gaussian/normal distribution? Thus, piecewise linear with mean and variance as changing properties. Why not set of polynomial models, as for example in \cite{chamroukhi2013joint}?

What is the best model for accelerometer data of human activities? --> Look to result of model-selecting papers.

Should we build a model of the data? (Gaussian distribution is also the model). And be able to reconstruct?

Why is it better (as Camci states) that a method makes no assumptions about the form of data/distribution of data? Is it that there are less parameters to estimate?
%----------------

Many methods describe and compare methods to construct classifier models for the classification of accelerometer data, such as \cite{kwapisz2011activity} and \cite{zhang2011optimal} (often using extracted features from the raw data signal).
In contrast, we could not find a clear characterization of accelerometer data obtained from human activities.
When the problem of temporal segmentation is regarded in this context, a formalization of the data under consideration is needed.
Some assume the data follows a piecewise linear set of segments with a mean and noise/variance modeled by a normal (Gaussian) distribution (such as \cite{camci2010change}).
Other approaches regard the data as a set of polynomials, which can be estimated by regression (such as \cite{chamroukhi2013joint}) and apply a form of model selection to each segment.

*** TODO: make a clear distinction between model types; similarity (distance based) or model based, as in \cite{yin2008sensor} ***

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
\section{Segmentation (SVM) method}
Overview of segmentation methods.
Collection of papers use relative and direct density-ratio estimation \cite{kawahara2009change, liu2013change}.

Why use \gls{svm} for density estimation? Look for justification, perhaps a review paper which compares/mentions SVM for temporal segmentation of (human activity type of) data?

Why use RBF/Gaussian kernel? Is it because of one-class SVM of because of form the data? Why not polynomial/linear?
\begin{itemize}
  \item ``Use RBF when relation between class and data is non-linear''
  \item ``RBF uses less paramters (C for penalty/soft margin and gamme for kernel width) than non-linear polynomial kernels''
  \item (arguments from \cite{zhang2011optimal} ``Optimal model selection for posture recognition in home-based healthcare'')
  \item Survey \cite{hodge2004survey} states RBF is similair to Gaussian Mixture model (page 25).
\end{itemize}



*** TODO: read \cite{mavroforakis2006geometric} and \cite{bennett2000duality} on geometry of SVMs ***

%-------------------------

Re-evaluatie \cite{guo2012adaptive}, uses (amongst others) a pca-based dimension-detection method (?).
Also uses model-selection as an intermediate step.


\subsection{Higher dimension mapping (including kernel)}
What does the mapping from the data-space to higher dimension looks like?

What is a RBF kernel?

What form has the higher dimensional space?

How do relations over data, such as distance, volume and noise, act in the higher dimensional space?

What is the kernel trick to not explicit do the mapping to the higher space?

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

*** Note: in \cite{mavroforakis2006geometric} it is explained why the inner-product between two vectors is a logical choice for the distance/similarity measure. ***


\section{Relation to other methods}

\subsection{Novelty/outlier detection}
*** Read \cite{hodge2004survey} ``A survey of outlier detection methodologies'' to compare with other methodologies. ***
\begin{itemize}
  \item One-class classification is referred to as ``Type 3'', with \emph{semi-supervised recognition or detection}.
  \item It explains why one-class can be beneficial over type-2 where negative examples needs to be provided.
  \item We are interested in type-3, so compare SVM with the others stated in this survey regarding type-3.
  \item Often refers to the convex hull of the data set. Link with geometric approach as in \cite{bennett2000duality,mavroforakis2006geometric}?
  \item It states that PCA and regression techniques are linear models and thus often are too simple for practical applications. SVMs try to find a hyperplane in higher dimensional space; linear models to implement complex class boundaries. It refers to \cite{tax1999support}.
\end{itemize}


\subsection{Scale parameter}
\subsection{Robust statistics / "M-Estimators"}
Is the method robust, in the sense that outliers have restricted impact on the quality.

What is the relation to M-Estimators (Wikipedia: ``M-estimators are a broad class of estimators, which are obtained as the minima of sums of functions of the data'', http://en.wikipedia.org/wiki/M-estimator)


%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
\section{Quality metrics}
As used in Camci: benefit, false alarm rate

Asymmetric test: feed data from front-to-back and back-to-front; how far are matched datapoint apart.

Model reconstruction error: try to reconstruct the simulated models, test similarity (BIC?). Is that a good measure? (If we are only interested in finding change points...)

