
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
\chapter{Introduction}

% the code below specifies where the figures are stored
\ifpdf
    \graphicspath{{1_introduction/figures/PNG/}{1_introduction/figures/PDF/}{1_introduction/figures/}}
\else
    \graphicspath{{1_introduction/figures/EPS/}{1_introduction/figures/}}
\fi

% ----------------------------------------------------------------------
%: ----------------------- introduction content ----------------------- 
% ----------------------------------------------------------------------



%: ----------------------- HELP: latex document organisation
% the commands below help you to subdivide and organise your thesis
%    \chapter{}       = level 1, top level
%    \section{}       = level 2
%    \subsection{}    = level 3
%    \subsubsection{} = level 4
% note that everything after the percentage sign is hidden from output



\section{Signal pre-processing and sensor fusion for timed patterns} % section 
%headings are printed smaller than chapter names
% intro

\section{Temporal segmentation}
This section will give an introduction and in-depth analysis of temporal 
segmentation. test. nog een test. test

\subsection{Aims of segmentation}
When processing and analyzing time series of data, e.g. motion measurements, 
stock market fluctuations or natural language, first a low-level division 
between the discriminative parts of the stream must be made. One can view this 
as splitting the series into the \emph{atoms}, which are the building blocks 
of the total stream. These building blocks will be the aggregation of 
non-overlapping, internally homogeneous segments \cite{himberg2001time}. This 
means that the data points inside a segment should have some resemblance 
relation to each other and their difference lies between some boundary. The 
process of segmenting can be viewed as a subproblem to context analysis of 
time series. Temporal segmentation is closely related to temporal clustering, 
although it is a stricter, and simpler, process. Whereby clustering only 
restricts the data points on their distance relation (as used in a Voronoi 
diagram), within a segment the data points must also be contiguous.

The task of segmentation can be performed in a manual 
matter, by cutting and labeling parts of the stream into coherent parts. This 
would require human (expert) knowledge and does not yield a clear cut because 
of ambiguity. With increasing storage abilities and easier motion capture 
systems, there is a desire for automated systems which perform the 
segmentation task unsupervised. Some algorithms used have the (often desired) 
side-effect of also clustering the segments, such that classes of segments can 
be discovered in the time series. These algorithms would not only be able to 
make a distinction between walking, sitting and walking, but would also 
recognize the reappearance of the walking activity.

\subsection{Formal definition}
Formally, temporal segmentation is dividing a time series $s$, which consists 
of $N$ samples $\mathbf{x}(1),\mathbf{x}(2),\dots,\mathbf{x}(N)$ from 
$\mathbf{R}^d$. Individual \emph{segments} are referenced by $s(a,b)$, 
consisting of the consecutive samples 
$\mathbf{x}(a),\mathbf{x}(a+1),\dots,\mathbf{x}(b)$, $a \le b$. Let $s_1 = 
s(a,b)$ and $s_2 = s(b+1,c)$ be two segments, then their concatenation is 
$s_1s_2 = s(a,c)$. A segmentation $S$ of $s$ consists of a sequence of $k$ 
non-empty segments $s_1s_2 \dots s_k = s$.
This notation has close resemblance to the notation as used in 
\cite{himberg2001time}.

As stated, informally each segment should be internally homogeneous. This can 
formally be measured with an cost function $F$, indication the heterogeneity 
of a segment. The overall aim is to minimize the cost $F$. The cost of a 
segment is a function from the data points and the number of data points $n = 
b - a + 1$ and is expressed as
\begin{equation} \label{eq:segment_cost}
\mathrm{cost}_F (s(a,b)) = F(\mathbf{x};n|\mathbf{x} \in s(a,b))
\end{equation}
The cost of a \emph{k-segmentation} $S$ is the summation of the costs of the 
$k$ segments:
\begin{equation} \label{eq:segmentation_cost}
\mathrm{Cost}_F (s_1 s_2 \dots s_k) = \sum_{i=1}^{k} \mathrm{cost}_F (s_k)
\end{equation}
With the objective of minimizing the cost function, the optimal 
$k$-segmentation $S_F^\mathit{opt}(s;k)$ is the segmentation with minimal 
$\mathrm{Cost}_F(s_1 s_2 \dots s_k)$ over all possible $k$-segmentations.

The cost function, to calculate the heterogeneity of a (set of) segment(s), 
can 
be any function. A simple and natural function would be the sum of variances 
of the segments. The overall cost function would then be
\begin{equation} \label{eq:cost_variances}
\mathrm{Cost}_V = \frac{1}{N} \sum_{i=i}^{k} \sum_{j=c_{i-1}+1}^{c_i} \| 
\mathbf{x}(j) - \mu_i \|^2
\end{equation} 
where $\mu_i$ is the mean vector of data points in segment $s_i$.

\subsection{Application in research fields}

[CHARACTERISTICS OF HUMAN MOTION]
temporal variability, invariance over time, metrics over actions.

[COMPUTER VISION]

[GRAPHICS/VIDEO]

[DATA-MINING]

[MODEL BASED]

To analyze time series it is often preferred to divide the stream in segments 
of correlated data. After dividing, each segment represent a period in time in 
which the same activity is performed. Or, stated otherwise, it results in 
transitions moments between activities.

Many fields of research have been active in the unsupervised segmentation of 
data. Many authors rely on a form of Principal Component Analysis (PCA), as 
used a.o. in \cite{barbivc2004segmenting}. Often PCA is used to reduce the 
dimensionality of the data being processed [REFERENCE] by only using the top 
$r$ dimensions to describe the data set. It is observed that data series of 
simple motions have a lower dimensionality then complexer motions. When a 
simple (repetitive) motion is about to end and fluently transforms in a new 
motion, there will be a window of time in which a high dimensionality will be 
present, due to the new motion. After this period of transition, the 
dimensionality will decrease, since only the new simple motion is present in 
the window of time. The first algorithm of \cite{barbivc2004segmenting} is 
based on this principle.

Given a set of data points, a lower dimensional hyperplane can by constructed 
to which the data points can be projected. This projection on a lower 
dimension introduces a error to the original position. When the error is 
fixed, less dimensions are needed for simple motions in which movements of 
body parts are highly correlated. For segments in which the data points are 
lesser correlated, e.g. because of transition state, a higher degree of 
dimensions of the hyperplane is needed to represent the data with equal error 
degree.

[SKIPPING]
By analyzing the derivative of the error rate, the algorithm is capable of 
selecting transition points and thus segmenting the data. When a data point 
$d_i$ is more than $k_\sigma=3$ times the standard deviation from the average 
over the previous data points $d_j, j<i$, then a cut is assigned to that frame.

A second approach in \cite{barbivc2004segmenting} uses the probabilistic 
variant of PCA (PPCA) to model the data set as a Gaussian distribution instead 
of ignoring the frames which do not fit in the subspace. Over windows of 
frames the mean and variance are calculated. In a forward manner the 
Mahalanobis distance of a new window of frames is calculated, which represents 
the likelihood of the new window belonging to the same segment as the original 
widow. When the distance decreases, the likelihood increases which happens 
when the motions in the becomes more homogeneous. When a peak in the distance 
is reached, the new window of frames indicates a heterogeneous collection of 
motions in the window and thus a low likelihood of membership and a indication 
of a transition. In order to distinct activities and subactivities (which 
require a subset of motions is a distinct activity) the algorithm is also 
processed backward over the data series.

The third algorithm in \cite{barbivc2004segmenting} is based on the 
observation that data points (frames) tend to form clusters in the space. 
These clusters are represented by $k$ Gaussian distributions for which each 
the Expectation-Maximization (EM) algorithm estimates the mean $m_j$, 
covariance matrix $\sum_{j}$ and prior $\pi_j$. With all the Gaussian 
distributions estimated, the data points are assigned to the cluster with the 
highest membership likelihood. When two consecutive frames $x_i$ and $x_{i+1}$ 
belong to different clusters, a transition of activities is recognized. Note 
that this algorithm succeeds in segmenting the data and also labels the 
similar simple activities.

A drawback in this system, and many others which implement a variant of the 
$k$-means algorithm, is that the number of clusters $k$ need to be 
predetermined. To cope with this, often the algorithm is performed multiple 
times for different values of $k$. Using some criteria, e.g. the Bayesian 
Information Criterion \cite{pelleg2000x} or the Davies-Bouldin Index which 
guides $k$-means clustering as used in \cite{krause2003unsupervised}.

\subsection{Principal Component Analysis}

\subsection{Segmentation as clustering}
In the previous section the discussed methods all relied on the stream of data 
points and tried to find cuts the discriminate between successive different 
type of activities. An other approach is to consider the tasks of segmentation 
as a type of clustering \cite{zhou2008aligned}. In clustering the objective is 
to assign labels, or classes, to all the data points indication a similar type 
of activity. A clustering $\mathcal{L}$ is thereby more informative then a 
segmentation but is also harder to produce.

A clustering $\mathcal{L}$ is generated from a sequence of elements 
$\mathbf{X}$ which is decomposed in $m$ disjoint segments, each belonging to 
one of the $k$ classes. A segment $\mathbf{Y}_i \hat{=} 
\mathbf{X}_{[s_i,s_{i+1})}$ is composed of frames from position $s_i$ to 
$s_{i+1}$. A vector $g_{ci} = 1$ indicates class membership if $\mathbf{Y}_i$ 
belongs to class $c$, otherwise $g_{ci} = 0$.

When regarding segmentation of human motion as a task of clustering the 
difficulty is to model the temporal variability of actions 
and defining a robust metric between temporal actions. To overcome this, 
\cite{zhou2008aligned} introduces Aligned Cluster Analysis (ACA), by minimizing
\begin{equation} \label{eq:ACA}
J_{\mathit{ACA}}(\mathbf{G},\mathbf{s}) = 
\sum_{c=i}^{K} \sum_{i=1}^{m} g_{ci} \mathit{dist}_c 
(\mathbf{X}_{[s_i,s_{i+1})})
\end{equation}

The characteristic of ACA is that is enables segments to span over different 
number of data points, whereas the standard kernel $k$-means algorithm results 
in equally sized segments. [!!! TRUE?!] The second difference is that the 
kernel used in $\mathit{dist}_c$ to measure the distance from a segment to the 
class which it is assigned to uses the Dynamic Time Alignment Kernel 
[REFERENCE?] to measure between time series.

\section{Temporal pattern recognition}

\subsection{Dynamic Time Warping}
Used to measure similarity between time sequences. Exact matching is 
high-cost, so approximations such as Minimum Bounding Rectangles are used.

\subsection{$k$-means clustering}
[EXPLAIN] divide data set $n$ into $k$ clusters.

Among many unsupervised clustering techniques, $k$-means is successfully 
applied to large data sets. It is simple to implement and linear in time 
complexity so computationally attractive \cite{jain1999data}. A drawback of 
the method is that the results of the algorithm greatly depends on the initial 
configuration (the data points which will act as centroids) and the number of 
cluster $k$ must be determined beforehand.

Generally, the $k$-means methods will minimize the squared error for a 
clustering $\mathcal{L}$ criterion which is defined as the distance from the 
data points the centroid for each cluster in $\mathcal{K}$. This is expressed 
as optimizing to a local optimum the energy function
\begin{equation} \label{eq:k-means energy}
e^2(\mathcal{K},\mathcal{L}) = 
\sum_{j=i}^{K}\sum_{i=1}^{n_j}\|\mathbf{x}_i^{(j)} - 
\mathbf{c}_j\|^2
\end{equation}

There are several limitation on the $k$-means method. One of these is that 
only spherical shapes of cluster can be generated. One of the extensions is 
kernel $k$-means \cite{scholkopf1998nonlinear}, which implicitly projects the 
data points to a higher dimension and thereby is able to form irregular shaped 
cluster.

\subsection{Self-organizing Map}

\subsection{Support Vector Machine}

\subsection{Na\"{i}ve Bayes}

\section{Unsupervised clustering of temporal patterns}




%: ----------------------- HELP: special characters
% above you can see how special characters are coded; e.g. $\alpha$
% below are the most frequently used codes:
%$\alpha$  $\beta$  $\gamma$  $\delta$

%$^{chars to be superscripted}$  OR $^x$ (for a single character)
%$_{chars to be suberscripted}$  OR $_x$

%>  $>$  greater,  <  $<$  less
%≥  $\ge$  greater than or equal, ≤  $\ge$  lesser than or equal
%~  $\sim$  similar to

%$^{\circ}$C   ° as in degree C
%±  \pm     plus/minus sign

%$\AA$     produces  Å (Angstrom)




% dextran, starch, glycogen continued
%Starch of plants and glycogen of animals consists of $\alpha$-1,4-glycosidic 
%glucose polymers \cite{lastname07}. See figure \ref{largepotato} for %a 
%%comparison of glucose polymer structure and chemistry. 

%Two references can be placed separated by a comma \cite{lastname07,name06}.

%: ----------------------- HELP: references
% References can be links to figures, tables, sections, or references.
% For figures, tables, and text you define the target of the link with \label{XYZ}. Then you call cross-link with the command \ref{XYZ}, as above
% Citations are bound in a very similar way with \cite{XYZ}. You store your references in a BibTex file with a programme like BibDesk.


%\figuremacro{largepotato}{A common glucose polymers}{The figure shows starch 
%granules in potato cells, taken from 
%%%\href{http://molecularexpressions.com/micro/gallery/burgersnfries/burgersnfries4.html}{Molecular
%% Expressions}.}

%: ----------------------- HELP: adding figures with macros
% This template provides a very convenient way to add figures with minimal code.
% \figuremacro{1}{2}{3}{4} calls up a series of commands formating your image.
% 1 = name of the file without extension; PNG, JPEG is ok; GIF doesn't work
% 2 = title of the figure AND the name of the label for cross-linking
% 3 = caption text for the figure

%: ----------------------- HELP: www links
% You can also see above how, www links are placed
% \href{http://www.something.net}{link text}

%\figuremacroW{lar gepotato}{Title}{Caption}{0.8}
% variation of the above macro with a width setting
% \figuremacroW{1}{2}{3}{4}
% 1-3 as above
% 4 = size relative to text width which is 1; use this to reduce figures





%: ----------------------- HELP: lists
% This is how you generate lists in LaTeX.
% If you replace {itemize} by {enumerate} you get a numbered list.


 


%: ----------------------- HELP: tables
% Directly coding tables in latex is tiresome. See below.
% I would recommend using a converter macro that allows you to make the table in Excel and convert them into latex code which you can then paste into your doc.
% This is the link: http://www.softpedia.com/get/Office-tools/Other-Office-Tools/Excel2Latex.shtml
% It's a Excel template file containing a macro for the conversion.

%\begin{table}[htdp]
%\centering
%\begin{tabular}{ccc} % ccc means 3 columns, all centered; alternatives are l, 
%%r
%
%{\bf Gene} & {\bf GeneID} & {\bf Length} \\ 
%% & denotes the end of a cell/column, \\ changes to next table row
%\hline % draws a line under the column headers
%
%human latexin & 1234 & 14.9 kbps \\
%mouse latexin & 2345 & 10.1 kbps \\
%rat latexin   & 3456 & 9.6 kbps \\
%% Watch out. Every line must have 3 columns = 2x &. 
%% Otherwise you will get an error.
%
%\end{tabular}
%\caption[title of table]{\textbf{title of table} - Overview of latexin genes.}
%% You only need to write the title twice if you don't want it to appear in 
%%bold in the list of tables.
%\label{latexin_genes} % label for cross-links with \ref{latexin_genes}
%\end{table}



% There you go. You already know the most important things.


% ----------------------------------------------------------------------



