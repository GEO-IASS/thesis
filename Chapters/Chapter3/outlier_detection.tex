% !TEX root = ../../main.tex
\section{Outlier and change detection}\label{sec:outlier_detection}

\subsection{General framework for outlier detection}
In \cite{schubert2012local} a general framework for outlier detection is proposed.
The authors compare existing methods and identified the common building block of the algorithms.
The focus is on unsupervised methods, using information from a local selection of data objects for the detection of outliers.
The following common algorithmic building blocks are identified:

\begin{enumerate}
  \item Context: a ``local'' context of an object $o$ for model building
  \item Model: the method used for building the model
  \item Reference: a ``reference'' context of object $o$ for model comparison
  \item Comparison: the method used for model comparison
  \item Normalization: a (global) normalization procedure
\end{enumerate}

Here the \emph{context} and \emph{reference} are sets of objects used for model building and model comparison, respectively.


\subsection{Regression, Classification, etc}

General framework of outlier detection, change detection in context of (simple?) regression, classification.

\subsection{M-estimators}
*** Move this section to somewhere in the end; is specification/extension, not base material ***
To make a method less sensitive to outliers (in the training data) techniques from robust statistics are applied.
The term \emph{robustness} has many interpretations, one of them that it ``signifies insensiÂ­tivity to small deviations from the [prior] assumptions [about the underlying situation]'', according to Huber \cite{huber2009robust}.
Methods have been proposed to apply robust statistics in the form of M-estimators to \glspl{svm}, such as \cite{choi2009least,chen2004m,suykens1999least}.

*** Question: do slack-variables (allowance of outliers) make SVM robust? ***