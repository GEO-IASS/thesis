% !TEX root = ../../main.tex
\section{Temporal Segmentation}\label{sec:literature_review_temporal_segmentation}

% \TODO{Is there any research of temporal segmentation on accelerometer/inertia signals? Note if there is/isnt}

% Give overview of used temporal segmentation methods. Problem is that it is not really well-defined.

% \begin{itemize}
%   \item Given overview of segmentation techniques, for times series data
%   \item Use different ``point-of-views'', or terms
%   \item ``Segmentation''
%   \item ``Change detection''
%   \item ``Novelty detection''
%   \item Specific view on \glspl{svm}
% \end{itemize}

This section gives an overview of the literature on temporal segmentation in the context of \gls{har}.
It provides a look on different implementations and methodologies.
A wide range of terms and subtle differences are used in the field, such as `segmentation', `change detection', `novelty detection' and `outlier detection'.
The following sections discuss these different methods.

\subsection{Segmentation based methods}\label{subsec:segmentation}

Methods that apply temporal segmentation on time series data can be roughly categorized in three different methods, as discussed by Avci \etal \cite{avci2010activity}.
In that survey the distinction between \emph{Top-Down}, \emph{Bottom-Up}, and \emph{Sliding-Window} approaches is based on the way data is processed by the algorithm.
Since we are interested in on-line change detection, the literature discussed in this section will mainly be forms of sliding-window algorithms.

In Keogh \etal \cite{keogh2001online} a comparison of the aforementioned approaches is made.
The research also introduces the \gls{swab} method, which combines the simple and intuitive approach of the sliding-window approach with the (semi-)global view over the data from the bottom-up method.
% This proposed method depends on user provided parameters, setting the maximum error when performing the algorithm.
Each segment is approximated by a \gls{plr}, within a certain error.
The user-provided error threshold controls the granularity and number of segments.

Other methods have been proposed, such as an adaptive threshold based on the signal energy by Guenterberg \etal \cite{guenterberg2009automatic}, the adaptive \gls{cusum}-based test by Alippi \etal \cite{alippi2006adaptive} and the \gls{mosum} by Hsu \cite{hsu2007mosum} in order to eliminate this user dependency.
The latter of these methods is able to process the accelerometer values directly, although better results are obtained when features of the signal are processed, as done in the first method \cite{guenterberg2009automatic}.
Here the signal energy, mean and standard deviation are used to segment activities and by adding all the axial time series together, the Signal-To-Noise ratio is increased, resulting in a robuster method.

The method of Guenterberg \etal \cite{guenterberg2009automatic} extracts features from the raw sensor signal to base the segmentation on other properties than the pure values.
The method of Bernecker \etal \cite{bernecker2012activity} uses other statistical properties, namely autocorrelation, to distinguish periodic from non-periodic segments.
Using the \gls{swab} method the self-similarity of a one-dimensional signal is obtained.
The authors claim that only a slight modification is needed to perform the method on multi-dimensional data.
After the segmentation phase, the method of Bernecker \etal \cite{bernecker2012activity} extracts other statistical features which are used in the classification phase.

The proposal of Guo \etal \cite{guo2012adaptive} dynamically determines which features should be used for the segmentation and simultaneously determines the best model to fit the segment.
For each of the three dimensions features such as the mean, variance, covariance, correlation, energy and entropy are calculated.
By extending the \gls{swab} method, for every frame a feature set is selected, using an enhanced version of \gls{pca}.
The research also considered the (Stepwise) Feasable Space Window as introduced by \cite{liu2008novel}, but since it results in a higher error rate than \gls{swab}, the latter was chosen to extend.
Whereas the above mentioned algorithms use a linear representation, this methods considers linear, quadratic and cubical representations for each segment.
This differs from other methods where the model is fixed for the whole time series, such as \cite{fuchs2010online}, which is stated to perform inferior on non-stationary time series, such as those from daily life.

The time series data from a sensor can be considered as being drawn from a certain stochastic process.
% Probabilistic models can be constructed on that signal, yielding in probabilistic and Bayesian based segmentation methods.
The \gls{cusum}-methods instead follows a statistical approach and relies on the log-likelihood ratio \cite{gustafsson1996marginalized} to measure the difference between two distributions.
To calculate the ratio, the probability density functions need to be calculated.
The method of Kawahara \etal \cite{kawahara2009change} proposes to estimate the ratio of probability densities (known as the \emph{importance}), based on the log likelihood of test samples, directly, without explicit estimation of the densities.
The method by Liu \etal \cite{liu2013change} uses a comparable dissimilarity measure using the \gls{kliep} algorithm.
They claim this results in a robuster method for real-world scenarios.
Although this is a model-based method, no strong assumptions (parameter settings) are made about the models.

The method of Adams and MacKay \cite{adams2007bayesian} builds a probabilistic model on the segment run length, given the observed data so far.
Instead of modeling the values of the data points, the length of segments as a function of time is modeled by calculating its posterior probability.
It uses a prior estimate for the run length and a predictive distribution for newly-observed data, given the data since the last change point.
This method differs from the approach of Guralnik and Srivastava \cite{guralnik1999event} in which change points are detected by a change in the (parameters of an) fitted model.
For each new data point, the likelihoods of being a change point and part of the current segment are calculated, without a prior model (and thus this is a non-Bayesian approach).
It is observed that when no change point is detected for a long period of time, the computational complexity increases significantly.

Another application of \gls{pca} is to characterize the data by determining the dimensionality of a sequence of data points.
The proposed method of Berbi\v{c} \etal \cite{barbivc2004segmenting} determines the number of dimensions (features) needed to approximate a sequence within a specified error.
With the observation that more dimensions are needed to keep the error below the threshold when transitions between actions occur, cut-points can be located and segments will be created.
The superior extension of their approach uses a Probabilistic \gls{pca} algorithm to model the data from dimensions outside the selected set with noise.

In the method by Himberg \etal \cite{himberg2001time} a cost function is defined over segments of data.
By minimizing the cost function it creates internally homogeneous segments of data.
A segment reflects a state in which the devices, and eventually its user, are.
The cost function can be any arbitrary function and in the implementation the sum of variances over the segments is used.
Both in a local and global iterative replacement procedure (as an alternative for the computationally hard dynamic programming algorithm) the best breakpoint locations $c_i$ for a pre-defined number of segments $1 \leq i \leq k$ are optimized.

As stated before, often methods obtain an implicit segmentation as a result of classification over a sliding window.
The method of Yang \etal \cite{yang2008distributed} explicitly performs segmentation and classification simultaneously.
It argues that the classification of a pre-segmented test-sequences becomes straightforward with many classical algorithms to choose from.
The algorithm matches test examples with the \emph{sparsest} linear representation of mixture subspace models of training examples, searching over different temporal resolutions.

The method of Chamroukhi \etal \cite{chamroukhi2013joint} is based on a \gls{hmm} and logistic regression.
It assumes a $K$-state hidden process with a (hidden) state sequence, each state providing the parameters (amongst which the order) for a polynomial.
The order of the model segment is determined by model selecting, often using the \gls{bic} or the similar \gls{aic} \cite{akaike1974new}, as in \cite{he2008activity}.

% Field of computer vision: \cite{zhou2008aligned}, \cite{li2007segmentation}.

% --- Segmentation ---

% ``An online algorithm for segmenting time series'' \cite{keogh2001online}. See~\ref{sec:appendix-C-online-keogh}. 538, 2001 \\
% ``Segmenting time series: A survey and novel approach'' \cite{keogh2004segmenting}. 242, 2004 \\

% ``Time series segmentation for context recognition in mobile devices'' \cite{himberg2001time}. 158, 2001 \\

% ``An Adaptive Approach for Online Segmentation of Multi-Dimensional Mobile Data'' \cite{guo2012adaptive}. 4, 2012 \\

% ``Joint segmentation of multivariate time series with hidden process regression for human activity recognition'' \cite{chamroukhi2013joint}. 0, 2013. See~\ref{sec:appendix-C-joint-segmentation}. \\

% ``Segmentation and Recognition of Motion Streams by Similarity Search'' \cite{li2007segmentation}. 29, 2007 \\

% ``Novel Online Methods for Time Series Segmentation'' \cite{liu2008novel}. 22, 2008 \\

% ``Distributed Segmentation and Classification of Human Actions Using a Wearable Motion Sensor Network'' \cite{yang2008distributed}. 44, 2008 \\

% ``An Automatic Segmentation Technique in Body Sensor Networks based on Signal Energy'' \cite{guenterberg2009automatic}. 14, 2009 \\

% ``Online Segmentation of Time Series Based on Polynomial Least-Squares Approximations'' \cite{fuchs2010online}. 24, 2010 \\

% ``Aligned Cluster Analysis for Temporal Segmentation of Human Motion'' \cite{zhou2008aligned}. 63, 2008 \\

% ``Segmenting motion capture data into distinct behaviors'' \cite{barbivc2004segmenting}. 270, 2004 \\


\subsection{CUSUM}\label{subsec:literature_review_temporal_segmentation_cusum}

% \TODO{Link this subsection with previous and next}
% Papers:
% \begin{itemize}
%   \item Use of Cumulative Sums of Squares for Retrospective Detection of Changes of Variance, 1994, 162 refs \cite{inclan1994use} \\
%   Implemented this algorithm.
%   \item An adaptive CUSUM-based test for signal change detection, 2006, 15 refs. \cite{alippi2006adaptive}
%   \item The MOSUM of squares test for monitoring variance changes \cite{hsu2007mosum}
% \end{itemize}

% \emph{Notes:}
% Non-Bayesian change detection algorithm (\ie no prior distribution believe available for the change time).
% The \gls{cusum} method is developed by Page \cite{page1954continuous} for the application of statistical quality control (it is also known as a control chart).
% Primary for detection of mean shift.
% The \gls{mosum} of squares test for monitoring variance changes \cite{hsu2007mosum}.
% Use of Cumulative Sums of Squares for Retrospective Detection of Changes of Variance \cite{inclan1994use}

An other often used approach in the statistical framework of change detection is the \gls{cusum} as introduced by Page \cite{page1954continuous}.
Originally used for quality control in production environments, its main function is to detect change in the mean of measurements \cite{basseville1993detection}.
It is a non-Bayesian method and thus has not explicit prior belief for the change points.
Many extensions to this method have been proposed.
Some focus on the change in mean, such the method of Alippi and Roveri \cite{alippi2006adaptive}.
Others apply the method the problems in which the change of variance is under consideration.
Among others are there the centered version of the cumulative sums, introduced by Brown, Durbin and Evans \cite{brown1975techniques} and the \gls{mosum} of squares by \cite{hsu2007mosum}.

The method of Incl\'{a}n and Tiao \cite{inclan1994use} builds on the centered version of \gls{cusum} \cite{brown1975techniques} to detect changes in variance.
The obtained results (when applied to stock data) are comparable with the application of \gls{mle}.
Using the batch \gls{icss} algorithm they are able to find multiple change points, offline while post-processing the data.
Whereas \gls{cusum} can be applied to search for a change in mean, the \gls{icss} is adapted to find changes in variance.
Let $C_k = \sum_{i=1}^k \alpha_t^2$ be the cumulative sum of squares for a series of uncorrelated random variables $\{\alpha_t\}$ of length $T$.
The centered (and normalized) sum of squares is defined as
\begin{equation}
  \begin{aligned}
  D_k = \frac{C_k}{C_T} - \frac{k}{T}, & & k = 1, \dots, T, & & \text{with } D_0 = D_T = 0.
  \end{aligned}
\end{equation}
For a series with homogeneous variance, the value of $D_k$ will oscillate around $0$.
This $D_k$ is used to obtain a \emph{likelihood ratio} for testing the hypothesis of one change against no change in the variance.
In case of a sudden change, the value will increase and exceed some predefined boundary with high probability.
By using an iterative algorithm, the method is able to minimize the masking effect of successive change points.
The proposal of \cite{cheng2009efficient} extends on the \gls{cusum}-based methods to find change points in mean and variance, by creating a more efficient and accurate algorithm.


One of the motivations for the \gls{icss} algorithm was the heavy computational burden involved with Bayesian methods, which need to calculate the posterior odds for the log-likelihood ratio testing.
The \gls{icss} algorithm avoids applying a function at all possible locations, due to the iterative search.
The authors recommend the algorithm for analysis of long sequences.


% \subsection{Change detection methods}\label{subsec:change_detection}
% \TODO{change order of this and temporal segmentation sections? So first change, then segmentation?} \\
% Whereas the above mentioned researches focus on \emph{segmentation}, many have focused on \emph{change detection}.
% Although these techniques are closely related, there is a subtle difference.
% In the case of \emph{change detection} to goal is to find, possibly unrelated, sudden change points in a signal \cite{takeuchi2006unifying}.
% In contrast, the goal of \emph{temporal segmentation} is to find homogeneous segments of data, which can be the result of multiple detected changes.

% The \gls{icss} by Incl\'{a}n and Tiao \cite{inclan1994use} is a statistical method which obtains results (when applied to stock data) comparable to \gls{mle} and Bayesian \TODO{Bayesian What?}.
% Whereas \gls{cusum} can be applied to search for a change in mean, the \gls{icss} is adapted to find changes in variance.
% It obtains a \emph{likelihood ratio} for testing the hypothesis of one change against no change in the variance.
% Using an iterative approach, all possible change points are considered.
% The proposal of \cite{cheng2009efficient} extends on the \gls{cusum}-based methods to find change points in mean and variance, by creating a more efficient and accurate algorithm.

% \Cref{sec:change_detection_time_series} discusses the problem of change detection in time series further, and gives a formal problem definition.

% \TODO{move CUSUM based techniques to this subsection}

\subsection{Novelty and Outlier detection}\label{subsec:novelty_and_outlier_detection}
In the case of time series data, many approaches rely on the detection of outlier, or novel, data objects.
An increase in outlier data objects indicates a higher probability of change, as discussed by Takeuchi and Yamanischi~\cite{takeuchi2006unifying}.
The concepts of novelty detection and outlier detection are closely related and often used interchangeable.

The algorithm by Ma and Perkins~\cite{markou2003novelty} uses \gls{svr} to model a series of past observations.
Using the regression, a matching function $V(t_0)$ is constructed which uses the residual value of $t_0$ to create an outlier (out-of-class) confidence value.
The method defines novel \emph{events} as a series of observations for which the outlier confidence is high enough.
In an alternative approach by the same authors~\cite{ma2003time} a classification method, in contrast with regression, is used to detect outlier observations.
The latter approach uses the \gls{nu-svm} algorithm by Sch\"olkopf \etal \cite{scholkopf1999support}, which is further discussed in \Cref{sec:literature_review_svm}

A extensive overview of novelty detection based on statistical models is given by Markou and Singh~\cite{markou2003novelty}.
In the second part of their review~\cite{markou2003novelty-neural}, novelty detection by a variety of neural networks is discussed.
The survey by Hodge and Austin~\cite{hodge2004survey} distinguishes between three types of outlier detection: \begin{inparaenum}[1)]
    \item unsupervised,
    \item supervised, and
    \item semi-supervised learning.
  \end{inparaenum}
As we will see in \Cref{sec:one_class_classification}, in this research we are interested in methods from Type 3, of which \gls{occ} is a member.
More theoretical background on outliers in statistical data is provided in the work by Barnett and Lewis~\cite{barnett1994outliers}.
