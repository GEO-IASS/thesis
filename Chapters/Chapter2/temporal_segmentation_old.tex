% !TEX root = ../../main.tex
\section{Temporal Segmentation}\label{sec:temporal_segmentation_old}

\begin{itemize}
  \item Given overview of segmentation techniques, for times series data
  \item Use different ``point-of-views'', or terms
  \item ``Segmentation''
  \item ``Change detection''
  \item ``Novelty detection''
  \item Specific view on \glspl{svm}
\end{itemize}

This section gives an overview of the literature on temporal segmentation in the context of \gls{har}.
It takes a look on different implementations and methodologies.
A wide range of terms and subtle differences are used in the field, such as `segmentation', `change detection', `novelty detection' and `outlier detection'.
These will be the categorical terms for which we discuss the literature.
Finally we will discuss other applications of \glspl{svm} in the context of these terms.

\subsection{Segmentation}\label{subsec:segmentation}
\TODO{This subsection is mainly from previous draft version}

\TODO{Create compact notation, one sentence per paper max}

Segmentation methods can roughly be categorized in three methods in the way the data is processed, as discussed by Avci \etal \cite{avci2010activity}:
\begin{itemize}
  \item \textbf{Top-down} methods iteratively divide the signal in segments by splitting at the best location.
  The algorithm starts with two segments and completes when a certain condition is met, such as when an error value or number of segments $k$ is reached.
  These methods process the data points recursively, which results in a complexity of $O(n^2k)$.
  \item \textbf{Bottom-up} methods are the natural complement to top-down methods.
  They start with creating $n/2$ segments and iteratively join adjacent segments while the value of a cost function for that operation is below a certain value.
  Given the average segment length $L$, the complexity of this method is $O(nL)$.
  \item \textbf{Sliding-window} methods are simple and intuitive for online segmenting purposes.
  It starts with a small initial subsequence of the time series.
  New data points are joined in the segment until the fit-error is above a threshold.
  Since the data is only processed very locally, these methods can yield in poor results \cite{keogh2001online}.
  The complexity is equal to the bottom-up approach, $O(nL)$, where $L$ is the average segment length.
  \item \textbf{\acrlong{swab}}, as introduced by Keogh \etal \cite{keogh2001online}, joins the ability of the sliding window mechanism to process time series online and the bottom-up approach the create superior segments in terms of fit-error.
  The algorithm processes the data in two stages.
  The first stage is to join new data points in the current segment created by a sliding window, and pass this to a buffer with space for a few segments.
  The buffer then processes the data Bottom-up and returns the first (left-most) segment as final segment.
  Because this second stage retains some (semi-)global view of the data, the results are comparative with normal Bottom-up.
  It is stated by Keogh \etal that the complexity of \gls{swab} is a small constant factor worse than that of regular Bottom-up.
\end{itemize}
It is clear that for the application of this research sliding-window and preferably \gls{swab}-based algorithms should be considered.

The \gls{swab} method proposed by Keogh \etal \cite{keogh2001online} is dependent on an user setting, providing the maximum error when performing both stages.
Each segment is approximated by using piecewise linear representation (PLR), an often used method.
The user provided error threshold controls the granularity and number of segments.
Other methods have been proposed, such as an adaptive threshold based on the signal energy by Guenterberg \etal \cite{guenterberg2009automatic}, the adaptive \gls{cusum}-based test by Alippi \etal \cite{alippi2006adaptive} and the \gls{mosum} by Hsu \cite{hsu2007mosum} in order to eliminate this user-dependency.
The latter of these methods is able to process the accelerometer values directly, although better results are obtained when features of the signal are processed, as done in the former method.
Here the signal energy, mean and standard deviation are used to segment activities and by adding all the axial time series together, the Signal-To-Noise ration is increased, resulting in a robuster method.

The method of Guenterberg \etal extracts features from the raw sensor signal to base the segmentation on other properties than the pure values.
The method of Bernecker \etal \cite{bernecker2012activity} uses other statistical properties, namely autocorrelation, to distinguish periodic from non-periodic segments.
Using the \gls{swab} method the self-similarity of a one-dimensional signal is obtained.
The authors claim that only a slight modification is needed to perform the method on multi-dimensional data.
After the segmentation phase, the method of Bernecker \etal extracts other statistical features which are used in the classification phase.

The proposal of Guo \etal \cite{guo2012adaptive} dynamically determines which features should be used for the segmentation and simultaneously determines the best model to fit the segment.
For each of the three dimensions features such as the mean, variance, covariance, correlation, energy and entropy are calculated.
By extending the \gls{swab} method, for every frame a feature set is selected, using an enhanced version of \gls{pca}.
The research also considered the (Stepwise) Feasable Space Window as introduced by \cite{liu2008novel}, but since it results in a higher error rate than \gls{swab} the latter was chosen to extend.
Whereas the before mentioned algorithms use a linear representation, this methods considers linear, quadratic and cubical representations for each segment.
This differs from other methods where the model is fixed for the whole time series, such as \cite{fuchs2010online}, which is stated to perform inferior on non-stationary time series such as daily life.

The time series data from a sensor can be considered as being drawn from a certain stochastic process.
Probabilistic models can be constructed on that signal, yielding in probabilistic and Bayesian based segmentation methods.
The \gls{cusum}-methods follows a statistical approach and relies on the log-likelihood ratio \cite{gustafsson1996marginalized} to measure the difference between two distributions.
To calculate the ratio, the probability density functions need to be calculated.
The method of Kawahara \etal \cite{kawahara2009change} proposes to estimate the ratio of probability densities (known as the \emph{importance}), based on the log likelihood of test samples, directly, without explicit estimation of the densities.
The method by Liu \etal \cite{liu2013change} uses a comparable dissimilarity measure using the \gls{kliep} algorithm.
They claim this results in a robuster approach for real-world scenarios.
Although this is a model-based method, no strong assumptions (parameter settings) are made on the models.

The method of Adams and MacKay \cite{adams2007bayesian} builds a probabilistic model on the segment run length, given the observed data so far.
Instead of modeling the values of the data points as a probabilistic distribution, the length of segments as a function if time is modeled by calculating the posterior probability.
It uses a prior estimate for the run length and a predictive distribution for newly-observed data, given the data since the last change point.
This method contrasts with the approach of Guralnik and Srivastava \cite{guralnik1999event} in which change points are detected by a change in the (parameters of an) underlying, observed, model.
For each new data point, the likelihoods of being a change point and part of the current segment are calculated, without a prior model (and thus is a non-Bayesian approach).
It is observed that when no change point is detected for a long period of time, the computational complexity increases significantly.

Another application of \gls{pca} is to characterize the data by determining the dimensionality of a sequence of data points.
The proposed method of Berbi\v{c} \etal \cite{barbivc2004segmenting} determines the number of dimensions (features) needed to approximate a sequence within a specified error.
With the observation that more dimensions are needed to keep the error below the threshold when transitions between actions occur, cut-points can be located and segments will be created.
The superior extension of their approach uses a Probabilistic \gls{pca} algorithm to incorporate the dimensions outside the selected set as noise.

In the method by Himberg \etal \cite{himberg2001time} a cost function is defined over segments of data which is to be minimized.
The cost functions thereby searches for internally homogeneous segments of data, reflecting states in which the devices and the user are.
The cost function can be any arbitrary function and in the implementation the sum of variances over the segments is used.
Both in a local and global iterative replacement procedure (as an alternative for the computationally hard dynamic programming algorithm) the best breakpoint locations $c_i$ for a pre-defined number of segments $1 \leq i \leq k$ are optimized.

Many methods obtain an implicit segmentation as a result of classification over a sliding window \TODO{add refs}.
The method of Yang \etal \cite{yang2008distributed} explicitly performs segmentation and classification simultaneously.
It argues that the classification of a pre-segmented test-sequences becomes straightforward with many classical algorithms to choose from.
The algorithm matches test examples with the \emph{sparsest} linear representation of mixture subspace models of training examples, searching over different temporal resolutions.

The method of Chamroukhi \etal \cite{chamroukhi2013joint} is based on a \gls{hmm} and logistic regression.
It assumes a $K$-state hidden process with a (hidden) state sequence, each state providing the parameters (amongst which the order) for a polynomial.
The order of the model segment is determined by model selecting, often using the \gls{bic} or the similar \gls{aic} \cite{akaike1974new}, as in \cite{he2008activity}.

Field of computer vision: \cite{zhou2008aligned}, \cite{li2007segmentation}.

--- Segmentation ---

% ``An online algorithm for segmenting time series'' \cite{keogh2001online}. See~\ref{sec:appendix-C-online-keogh}. 538, 2001 \\
% ``Segmenting time series: A survey and novel approach'' \cite{keogh2004segmenting}. 242, 2004 \\

% ``Time series segmentation for context recognition in mobile devices'' \cite{himberg2001time}. 158, 2001 \\

% ``An Adaptive Approach for Online Segmentation of Multi-Dimensional Mobile Data'' \cite{guo2012adaptive}. 4, 2012 \\

% ``Joint segmentation of multivariate time series with hidden process regression for human activity recognition'' \cite{chamroukhi2013joint}. 0, 2013. See~\ref{sec:appendix-C-joint-segmentation}. \\

``Segmentation and Recognition of Motion Streams by Similarity Search'' \cite{li2007segmentation}. 29, 2007 \\

% ``Novel Online Methods for Time Series Segmentation'' \cite{liu2008novel}. 22, 2008 \\

% ``Distributed Segmentation and Classification of Human Actions Using a Wearable Motion Sensor Network'' \cite{yang2008distributed}. 44, 2008 \\

% ``An Automatic Segmentation Technique in Body Sensor Networks based on Signal Energy'' \cite{guenterberg2009automatic}. 14, 2009 \\

% ``Online Segmentation of Time Series Based on Polynomial Least-Squares Approximations'' \cite{fuchs2010online}. 24, 2010 \\

``Aligned Cluster Analysis for Temporal Segmentation of Human Motion'' \cite{zhou2008aligned}. 63, 2008 \\

% ``Segmenting motion capture data into distinct behaviors'' \cite{barbivc2004segmenting}. 270, 2004 \\

\subsection{Change detection}\label{subsec:change_detection}
\TODO{change order of this and temporal segmentation sections? So first change, then segmentation?} \\
Whereas the above mentioned researches focus on \emph{segmentation}, many have focused on \emph{change detection}.
Although these techniques are closely related, there is a subtle difference.
In the case of \emph{change detection} to goal is to find, possibly unrelated, sudden change points in a signal \cite{takeuchi2006unifying}.
In contrast, the goal of \emph{temporal segmentation} is to find homogeneous segments of data, which can be the result of multiple detected changes.

The \gls{icss} by Incl\'{a}n and Tiao \cite{inclan1994use} is a statistical method which obtains results (when applied to stock data) comparable to \gls{mle} and Bayesian \TODO{Bayesian What?}.
Whereas \gls{cusum} can be applied to search for a change in mean, the \gls{icss} is adapted to find changes in variance.
It obtains a \emph{likelihood ratio} for testing the hypothesis of one change against no change in the variance.
Using an iterative approach, all possible change points are considered.
The proposal of \cite{cheng2009efficient} extends on the \gls{cusum}-based methods to find change points in mean and variance, by creating a more efficient and accurate algorithm.

\Cref{sec:change_detection_time_series} discusses the problem of change detection in time series further, and gives a formal problem definition.

\TODO{move CUSUM based techniques to this subsection}


--- Change detection ---

``A unifying framework for detecting outliers and change points from time series'' \cite{takeuchi2006unifying}. 87, 2006 \\

% ``Bayesian online changepoint detection'' \cite{adams2007bayesian}. 85, 2007 \\

% ``An adaptive cusum-based test for signal change detection'' \cite{alippi2006adaptive}. 18, 2006 \\

% ``An efficient algorithm for estimating a change-point'' \cite{cheng2009efficient}. 5, 2009 \\

% ``The marginalized likelihood ratio test for detecting abrupt changes'' \cite{gustafsson1996marginalized}. 80, 1996 \\

% ``The MOSUM of squares test for monitoring variance changes'' \cite{hsu2007mosum}. 4, 2007 \\

% ``Use of cumulative sums of squares for retrospective detection of changes of variance'' \cite{inclan1994use}. 643, 1994 \\

% ``Change-point detection in time-series data by direct density-ratio estimation'' \cite{kawahara2009change}. 52, 2009 \\
``Sequential change-point detection based on direct density-ratio estimation'' \cite{kawahara2012sequential}. 22, 2012 \\
% ``Change-point detection in time-series data by relative density-ratio estimation'' \cite{liu2013change}. 11, 2013 \\

``Change point detection in time series data using support vectors'' \cite{camci2010change}. 3, 2010 \\

\subsection{Novelty detection}\label{subsec:novelty_detection}

--- Novelty detection ---

``Online novelty detection on temporal sequences'' \cite{ma2003online}. 146, 2003 \\
``Time-series novelty detection using one-class support vector machines'' \cite{ma2003time}. 78, 2003 \\

``Novelty detection: a review—part 1: statistical approaches'' \cite{markou2003novelty}. 697, 2003 \\

``Support Vector Method for Novelty Detection'' \cite{scholkopf1999support}. 337, 1999 \\

\subsection{Outlier detection}\label{subsec:outlier_detection}

--- Outlier detection ---
``A unifying framework for detecting outliers and change points from time series'' \cite{takeuchi2006unifying}. 87, 2006 \\

``Outliers in statistical data'' \cite{barnett1994outliers} (book). 3745, 1994 \\

``A survey of outlier detection methodologies'' \cite{hodge2004survey}. 791, 2004 \\

``Local outlier detection reconsidered: a generalized view on locality with applications to spatial, video, and network outlier detection'' \cite{schubert2012local}. 2, 2012 \\


% --- SVMs ---

% ``Change point detection in time series data using support vectors'' \cite{camci2010change}. 3, 2010 \\

% ``Time-series novelty detection using one-class support vector machines'' \cite{ma2003time}. 78, 2003 \\

% ``Support vector domain description'' \cite{tax1999support}. 907, 1999
% ``Support vector data description applied to machine vibration analysis'' \cite{tax1999supportdata}. 76, 1999 \\

% ``Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine'' \cite{anguita2012human}. 13, 2012 \\

% ``Support vector machines in remote sensing: A review'' \cite{mountrakis2011support}. 150, 2011 \\

% ``Sensor-based abnormal human-activity detection'' \cite{yin2008sensor}. 74, 2008 (builds one-class SVM of all normal traces) \\