% !TEX root = ../../main.tex
\section{Model Construction: Incremental SVDD}\label{sec:method_model_construction}
After the data is collected (or in the case of the artificial data sets: generated), we construct an online incremental sliding window algorithm.
We follow the \gls{svcpd} method by Camci \cite{camci2010change} and the \gls{isvdd} implementation by Tax and Laskov \cite{tax2003online}.
The latter algorithm combines the techniques of online, unsupervised, and incremental learning methods with the earlier introduced \gls{oc-svm} algorithm \gls{svdd}.
The method is initialized with a fixed window length and then at every step a new data object is added to and the oldest data object is removed from this working set.
The model construction phase can be regarded as the first stage of the unifying framework by Takeuchi and Yamanishi \cite{takeuchi2006unifying}.

Using the following abstract form of the \gls{svm} optimization problem, the extension of the incremental \gls{svm} to the \gls{svdd} can be carried out:
\begin{equation}
  \operatorname*{max}_\mu \operatorname*{min}_{\substack{
    0 \le x \le C \\
    \vectorsym{a}^T \vectorsym{x} + b = 0}
  } : W = -\vectorsym{c}^T\vectorsym{x} + \frac{1}{2}\vectorsym{x}^T K\vectorsym{x} + \mu(\vectorsym{a}^T\vectorsym{x} + b),
\end{equation}
where $\vectorsym{c}$ and $\vectorsym{a}$ are $n \times 1$ vectors, $K$ is a $n \times n$ matrix and $b$ is a scalar.
The \gls{svdd} implementation of this abstract form is set by the parameters $\vectorsym{c}=\operatorname*{diag}(K)$, $\vectorsym{a} = \vectorsym{y}$ and $b=1$.
The procedure for the incremental version has two operations for a data object $k$: add to and remove from the working set.
When a data object $k$ added, its weight $x_k$ is initially set to $0$.
In case of an object removal, the weight is forced to be $x_k=0$.
Both the operations conclude with the recalculation  of $\mu$ and the weights $\vectorsym{x}$ for all the objects, in order to obtain the optimal solution for the enlarged or reduced data set.
These two operations are the basis for \gls{isvdd}.
With the data points ordered by timestamp, new data objects are added to and old data objects are removed from the working set.

The size of the initial window of data objects has a lower bound determined by the hyperparameter $C$ (\Cref{eq:svdd_objective}).
Because of the equality constraint $\sum_{i=1}^n a_i x_i = 1$ and the box constraint $0 \le x_i \le C$, the number of objects in the working set must be at least $\ceil{\frac{1}{C}}$.
Thus the algorithm is initialized by selection the first $\ceil{\frac{1}{C}}$ objects for the working set.
In every loop of the algorithm the number of elements added are at least enough to keep $\ceil{\frac{1}{C}}$ objects in the window, after removing the oldest objects.
The optimality of the algorithm is proved by analysis of the \gls{kkt} conditions \cite{tax2003online}.

From experiments it shows that the (online) \gls{isvdd} method results in less false alarms than the static \gls{svdd}.
An explanation for this is that \gls{isvdd} follows the changing data distribution, \ie small changes over time.
Such changes are a drift in mean or increase in frequency.
The \gls{isvdd} algorithm continuously re-models the \gls{svm} representation.
This re-modeling of the \gls{svm} representation is beneficial for our purpose, since we are only interested in sudden changes between activities.
The following section will discuss the features extracted from the constructed \gls{oc-svm} model at every loop-step of the algorithm, and \Cref{sec:method_change_detection} describes how these features are used to detect change.