% Section Signal Pre-processing
% !TEX root = ../../main.tex

% \section{Literature review}
% Look into earlier application mentioned in the literature to this kind of  problems. Look for similarities in the problem and address where the  techniques used fail or are not applicable.

% *** Most of the activity recognition techniques can be classified in two  different types.
% Some rely on a state-space model, in which the activities to be recognized are represented in a statistical manner.
% Bayesian networks, Finite State Machines and Hidden Markov Models can be considered among those.
% Other techniques process the directly as a pattern recognition task, such as  Support Vector Machines, Neural Networks, Dynamic Time Warping, and Bayes and K-means clustering.

% \subsection{Segmentation}
% Describe the different approaches to segmentation.
% Group different viewpoints, e.g. cut-points, change detection, segmentation, etc.


\section{Literature review}
This section will provide a review of the existing literature and research of the different subjects discussed in this thesis.
First the different approaches to mere segmentation of signals is discussed, followed by *** other techniques. ***

\subsection{Segmentation}
One of the first stages in analyzing the sensor data time series is to segment the signal.
The goal is to set cut-points in the signal such that all the data points in between consecutive cut-points have some homogeneous property among them. *** "organizing data into homogeneous groups where the within-group-object similarity is minimized and the between-group-object dissimilarity is maximized." ***
This results in the more abstract notion of change (point) detection.
The general concern is to detect the occurrence and time identification of changes in the signal, often expressed as changes in the probability distribution of the time series.
This literature review will discuss the different types of change detection as applied in the research.
To each applications question about the methods, complexity and usability etc. are asked.
On account of usability, it is desired to have a successful segmentation algorithm with minimal user interaction, e.g. defining thresholds and error rates.
** something about what is considered, e.g. complexity, measurements, e.g. ***

Following the organization of \cite{warren2005clustering}, research can be structured in three groups.
The data being processed determines the classification, which can be the raw data, indirectly via features extracted from the raw data or indirectly via models extracted from the raw data.

One of the more simple and directs methods is to Piecewise Linear Represent the signal, while keeping the approximation error below a certain value, as discussed in \cite{keogh2001online}.
As with many algorithms, PLR can be applied to a batch of or real time stream input data (also known as off-line and on-line analyzes, respectively).
Keogh et al. \cite{keogh2001online} propose a method which combines these different techniques to form a Sliding Window and Bottom-up (SWAB) analyzes.
The algorithm works by first, per window of data points, creating for every two data points a segment.
Neighboring segments are joined together, as long as the approximation error is below the provided value.
This analyzes can be used as a starting point to further analyze the data segments.
Although Keogh et al. apply the SWAB algorithm only direct on the raw data, it is easily imagined how to apply it to extracted features of the signal, e.g. the energy or mean.
In its default application, the method yields little information about each segment; it is merely a linear representation.
The SWAB algorithm works by segmenting parts of the data series by linear approximation while keeping the error below a certain rate.
This rate needs to be user defined (e.g. by an expert), which makes it not possible to apply directly on any signal.
This method only divides the signal into segments, which representations can be processed further to form clusters or related segments.
The complexity of this method is relatively low, $O(Ln)$, where $L$ is the expected segment length and $n$ the number of data points.

The task of segmenting any form of data is made easier when the number of segments is beforehand known.
Many authors apply the well-known \emph{k-means} clustering *** refs needed *** on this type of problems.
An incremental error-minimizing version can determine the optimal number of segments (or actually, clusters in this case).
The proposed method of \cite{himberg2001time}, Global Iterative Replacement, resembles the method of Keogh et al., in the sense that it merges segments.
The algorithm is applied on two different data sets.
The first data sets can be considered as extracted features from sensor data, whilst the second data set works on model properties and thus can give a richer contextual segmentation.
For both types of data sets a cost function is defined, which represents the internal heterogeneity of a segment (e.g. the variance of the data within a segment).
The difference with the method of Keogh et al. is that with GIR the number of segments is known beforehand and during the algorithm this number is, through splits and merges, kept constant.
By running multiple instances of the GIR algorithm with each an increasing number of $k$ segments (bounded by user provided values), the optimal number and location of cut points is determined.

Other approaches form a higher level analyses of the raw data.
As used in \cite{barbivc2004segmenting}, from the field of computer graphics, the dimensionality of the data can be used to distinguish between actions.
Using Principal Component Analyses, Barbi{\v{c}} et al. use the dimensionality required to project the data points on a hyperplane while keeping the error below a certain rate as a measure of similarity.
The error threshold is fixed as a range of three times the standard deviation from the mean of the error.
In an extension of their method, using Probabilistic PCA \cite{tipping1999probabilistic}, the Mahanalobis distance \cite{duda1995pattern} for each frame represents similarity of concurrent motions.
By traversing the Mahanalobis distance as a function of the frames, cut-points are placed on specific points of interest.
*** NOTE to self: thus this method can segment (and group) unknown and new activities? ***
In a third method the authors use a Gaussian Mixture Model *** elaborate on this one [does it segment of direct classify?] ***.
Using this segmenting, and comparing the segments with earlier gathered representations of motions, the PPCA method gets the best results, with precision of 92\% and recall of 95\%.
The test subjects wore 14 sensors on joints of the body and each frame is represented as a 56-dimensional vector.
The test considered seven simple human motions: walking, running, sitting down, forward jumping, climbing, arm stretching and punching.

It is very common the use a higher level of features extracted from the raw data.
The approach of Zhou et al. \cite{zhou2008aligned} maps the data points to a higher dimension by using a kernel function.
The proposed method, Aligned Cluster Analysis, extends kernel $k$-means clustering on this higher dimensional data set.
Using the Dynamic Time Aligned Kernel (DTAK) metric, similarity between segments is measured.
Due to the properties of $k$-means clustering, discovering an unknown number of segments in the data set requires and iterative approach.


*** motifs ***


*** HMM ***

*** Finite mixture models ***

*** Infinite mixture models ***


*****
List of all papers, shortly categorized
\begin{itemize}
  \item ``Discovering characteristic actions from on-body sensor data'' \cite{minnen2006discovering}, cited: 63. Motifs, HHM, DTW, 87\% accuracy. Known motifs.
  \item ``Recognition of human activities using layered hidden Markov models'' \cite{perdikis2008recognition}, cited: 3. HMM, layerd (primitive and abstract actions). Uses vision for workplace-activities. Direct classification
  \item ``Accelerometer-Based Gait Analysis, A survey'', \cite{derawi2010accelerometer}, cited: 4. Compares methods to distinguish walking of normal, fast, slow. Focus on gait, but compares classification methods such as SVM, PCA, KSOM.
  \item ``An Automatic Segmentation Technique in Body Sensor Networks based on Signal Energy'', \cite{guenterberg2009automatic}, cited: 13. Automatic segmenting, adaptive threshold. Pure segmenting, no classification. Nice and clean method? Weakness: single action segmented as multiple. Used multiple sensors.
  \item ``Towards HMM based Human Motion Recognition using MEMS Inertial Sensors'', \cite{shi2009towards}, cited: 13. Uses HHM, Fourier transform for features. 5 fixed activities, trained HMM. Correct rates from 90\% to 100\%. Classification, not just segmentation
  \item ``Change-Point Detection in Time-Series Data by Direct Density-Ratio Estimation'', \cite{kawahara2009change}, cited: 41. Non-parametric approach, online method, no strong model assumptions. Uses multiple datasets, origin referenced. Much follow-up work done.
  \item ``Unsupervised, Dynamic Identification of Physiological and Activity Context in Wearable Computing'', \cite{krause2003unsupervised}, cited: 106(!). Combination of KSOM, iterative K-means clustering, transient states removal (using markov model). Online algorithm.
  \item ``Activity Recognition using Cell Phone Accelerometers'', \cite{kwapisz2011activity}, cited: 122 (!).
\end{itemize}





*** Elaborate on measures, e.g. precision, recall, confusion matrix etc.  ***
*** Precision is defined as the ratio of reported correct cuts versus the total number of reported cuts. Recall is defined as the ratio of reported correct cuts versus the total number of correct cuts. The closer precision and recall are to 1, the more accurate the algorithm is. ***